{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ebe646-bbf7-4479-b953-1f070effa587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scikit-learn 1.4.2\n",
      "Uninstalling scikit-learn-1.4.2:\n",
      "  Successfully uninstalled scikit-learn-1.4.2\n",
      "Found existing installation: numpy 2.2.4\n",
      "Uninstalling numpy-2.2.4:\n",
      "  Successfully uninstalled numpy-2.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall scikit-learn numpy -y\n",
    "!pip install numpy scikit-learn --upgrade --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7725f777-d248-442e-bc0c-afb1b07684ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rtdl in c:\\users\\user\\anaconda3\\lib\\site-packages (0.0.13)\n",
      "Requirement already satisfied: numpy<2,>=1.18 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rtdl) (1.26.4)\n",
      "INFO: pip is looking at multiple versions of rtdl to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting rtdl\n",
      "  Using cached rtdl-0.0.13-py3-none-any.whl.metadata (1.0 kB)\n",
      "  Using cached rtdl-0.0.12-py3-none-any.whl.metadata (1.0 kB)\n",
      "  Using cached rtdl-0.0.10-py3-none-any.whl.metadata (1.0 kB)\n",
      "  Using cached rtdl-0.0.9-py3-none-any.whl.metadata (1.0 kB)\n",
      "INFO: pip is still looking at multiple versions of rtdl to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached rtdl-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "  Using cached rtdl-0.0.7-py3-none-any.whl.metadata (934 bytes)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached rtdl-0.0.6-py3-none-any.whl.metadata (934 bytes)\n",
      "  Using cached rtdl-0.0.5-py3-none-any.whl.metadata (934 bytes)\n",
      "  Using cached rtdl-0.0.4-py3-none-any.whl.metadata (934 bytes)\n",
      "  Using cached rtdl-0.0.3-py3-none-any.whl.metadata (934 bytes)\n",
      "  Using cached rtdl-0.0.2-py3-none-any.whl.metadata (995 bytes)\n",
      "  Using cached rtdl-0.0.1-py3-none-any.whl.metadata (995 bytes)\n",
      "\n",
      "The conflict is caused by:\n",
      "    rtdl 0.0.13 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.13 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.13 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.12 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.13 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.10 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.13 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.9 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.13 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.8 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.13 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.7 depends on torch<2 and >=1.6\n",
      "    rtdl 0.0.13 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.6 depends on torch<2 and >=1.6\n",
      "    rtdl 0.0.13 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.5 depends on torch<2 and >=1.6\n",
      "    rtdl 0.0.13 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.4 depends on torch<2 and >=1.6\n",
      "    rtdl 0.0.13 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.3 depends on torch<2 and >=1.6\n",
      "    rtdl 0.0.13 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.2 depends on torch<2 and >=1.6\n",
      "    rtdl 0.0.13 depends on torch<2 and >=1.7\n",
      "    rtdl 0.0.1 depends on torch<2 and >=1.6\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install rtdl==0.0.1, rtdl==0.0.10, rtdl==0.0.12, rtdl==0.0.13, rtdl==0.0.2, rtdl==0.0.3, rtdl==0.0.4, rtdl==0.0.5, rtdl==0.0.6, rtdl==0.0.7, rtdl==0.0.8 and rtdl==0.0.9 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "!pip install -U rtdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0c02fb-9f89-4fa4-8e5b-ff4c4fe4a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d96d052-b887-4e09-a43b-42e806365410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_30332\\639464490.py:2: DtypeWarning: Columns (385) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  evaluation_df=pd.read_csv(\"C:/Users/user/Desktop/신용카드고객/evaluation_df.csv\")\n"
     ]
    }
   ],
   "source": [
    "train_sampled_df=pd.read_csv(\"C:/Users/user/Desktop/신용카드고객/train_sampled_df.csv\")\n",
    "evaluation_df=pd.read_csv(\"C:/Users/user/Desktop/신용카드고객/evaluation_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29c167e-524a-42d9-9040-b8c67f5da84a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sampled_df after preprocessing:\n",
      "   Unnamed: 0    기준년월            ID  남녀구분코드   연령 Segment  회원여부_이용가능  \\\n",
      "0      599866  201808  TRAIN_199866       2  30대       E          1   \n",
      "1      427635  201808  TRAIN_027635       1  50대       E          0   \n",
      "2     1833852  201811  TRAIN_233852       2  40대       E          1   \n",
      "3      690821  201808  TRAIN_290821       1  40대       E          1   \n",
      "4     1098632  201809  TRAIN_298632       2  30대       D          1   \n",
      "\n",
      "   회원여부_이용가능_CA  회원여부_이용가능_카드론  소지여부_신용  ...  변동률_RV일시불평잔  변동률_할부평잔  변동률_CA평잔  \\\n",
      "0             1              0        1  ...     0.999998  1.987409  0.999998   \n",
      "1             0              0        1  ...     0.999998  0.999998  0.999998   \n",
      "2             1              1        1  ...     0.999998  0.999998  0.999998   \n",
      "3             1              1        1  ...     0.999998  0.904525  0.999998   \n",
      "4             1              1        1  ...     0.952195  0.604032  0.999998   \n",
      "\n",
      "   변동률_RVCA평잔  변동률_카드론평잔  변동률_잔액_B1M  변동률_잔액_일시불_B1M  변동률_잔액_CA_B1M  \\\n",
      "0    0.999998   0.999998    0.147471       -0.116887            0.0   \n",
      "1    0.999998   0.999998    0.000000        0.000000            0.0   \n",
      "2    0.999998   0.999998    0.000000        0.000000            0.0   \n",
      "3    0.999998   0.999998   -0.025178        0.102291            0.0   \n",
      "4    0.999998   0.999998    0.199584        0.085968            0.0   \n",
      "\n",
      "   혜택수혜율_R3M  혜택수혜율_B0M  \n",
      "0   0.000000   0.000000  \n",
      "1   0.000000   0.000000  \n",
      "2   0.000000   0.000000  \n",
      "3   3.111331   3.220928  \n",
      "4  -0.084577   0.075305  \n",
      "\n",
      "[5 rows x 853 columns]\n",
      "evaluation_df after preprocessing:\n",
      "   Unnamed: 0    기준년월            ID  남녀구분코드   연령 Segment  회원여부_이용가능  \\\n",
      "0      140607  201807  TRAIN_140607       1  60대       E          1   \n",
      "1      615413  201808  TRAIN_215413       1  50대       E          1   \n",
      "2     2128921  201812  TRAIN_128921       1  50대       E          1   \n",
      "3      494497  201808  TRAIN_094497       2  40대       D          1   \n",
      "4     1814277  201811  TRAIN_214277       2  30대       E          1   \n",
      "\n",
      "   회원여부_이용가능_CA  회원여부_이용가능_카드론  소지여부_신용  ...  변동률_RV일시불평잔  변동률_할부평잔  변동률_CA평잔  \\\n",
      "0             1              1        1  ...     0.999998  0.999998  0.999998   \n",
      "1             1              0        1  ...     0.999998  0.901223  0.999998   \n",
      "2             1              1        1  ...     0.999998  0.999998  0.999998   \n",
      "3             1              0        1  ...     0.999998  1.990905  1.002733   \n",
      "4             1              1        1  ...     0.999998  0.000000  0.999998   \n",
      "\n",
      "   변동률_RVCA평잔  변동률_카드론평잔  변동률_잔액_B1M  변동률_잔액_일시불_B1M  변동률_잔액_CA_B1M  \\\n",
      "0    0.999998   0.999998    0.000000        0.000000        0.00000   \n",
      "1    0.999998   0.999998   -0.009084       -0.050106        0.00000   \n",
      "2    0.999998   0.999998    0.000000        0.000000        0.00000   \n",
      "3    0.999998   1.453043   -0.153078        0.000000       -0.07013   \n",
      "4    0.999998   0.999998   -0.025110       -0.140781        0.00000   \n",
      "\n",
      "   혜택수혜율_R3M  혜택수혜율_B0M  \n",
      "0   0.000000   0.000000  \n",
      "1   4.216123   4.244498  \n",
      "2   0.000000   0.000000  \n",
      "3   0.000000   0.000000  \n",
      "4   1.732865   2.974604  \n",
      "\n",
      "[5 rows x 853 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_df(df):\n",
    "    \"\"\"\n",
    "    train_sampled_df와 evaluation_df에 동일한 전처리를 적용하는 함수\n",
    "    \"\"\"\n",
    "    # 1. 업종 목록 결측치 처리\n",
    "    industry_list = [\n",
    "        '_3순위여유업종', '_3순위납부업종', '_2순위여유업종', '_3순위교통업종', '_2순위납부업종',\n",
    "        '_1순위여유업종', '_2순위교통업종', '_3순위쇼핑업종', '_1순위납부업종', '_1순위교통업종',\n",
    "        '_2순위쇼핑업종', '_3순위업종', '_1순위쇼핑업종', '_2순위업종', '_1순위업종'\n",
    "    ]\n",
    "    for industry in industry_list:\n",
    "        if industry in df.columns:\n",
    "            df[industry] = df[industry].fillna('없음')\n",
    "\n",
    "    # 2. 불필요한 열 삭제\n",
    "    columns_to_drop = [\n",
    "        '연체일자_B0M', '최종카드론_대출일자', '최종카드론_신청경로코드', '최종카드론_금융상환방식코드',\n",
    "        'RV신청일자', 'OS구분코드'\n",
    "    ]\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "    # 3. 가입통신회사코드 처리\n",
    "    if '회원여부_이용가능' in df.columns and '이용금액_R3M_신용' in df.columns:\n",
    "        df['가입통신회사코드'] = np.where(\n",
    "            (df['회원여부_이용가능'] == 'N') | (df['이용금액_R3M_신용'] == 0),\n",
    "            '미가입',\n",
    "            df['가입통신회사코드'].fillna('Unknown')\n",
    "        )\n",
    "\n",
    "    # 4. 직장시도명 처리\n",
    "    if '거주시도명' in df.columns:\n",
    "        df['직장시도명'] = np.where(\n",
    "            df['직장시도명'].isna() & df['거주시도명'].notna(),\n",
    "            df['거주시도명'],\n",
    "            df['직장시도명'].fillna('Unknown')\n",
    "        )\n",
    "\n",
    "    # 5. RV전환가능여부 처리\n",
    "    if '소지여부_신용' in df.columns and '이용금액_R3M_신용' in df.columns:\n",
    "        df['RV전환가능여부'] = np.where(\n",
    "            (df['이용금액_R3M_신용'] == 0) | (df['소지여부_신용'] == 'N'),\n",
    "            'N',\n",
    "            df['RV전환가능여부'].fillna('Unknown')\n",
    "        )\n",
    "\n",
    "    # 6. _1순위신용체크구분 처리\n",
    "    if '_1순위업종' in df.columns and '이용금액_R3M_신용' in df.columns:\n",
    "        df['_1순위신용체크구분'] = np.where(\n",
    "            df['이용금액_R3M_신용'] == 0,\n",
    "            '미사용',\n",
    "            np.where(\n",
    "                df['_1순위신용체크구분'].isna() & df['_1순위업종'].notna() & (df['_1순위업종'] != '없음'),\n",
    "                '신용',\n",
    "                df['_1순위신용체크구분'].fillna('미사용')\n",
    "            )\n",
    "        )\n",
    "        # _1순위와 _2순위 상호작용\n",
    "        df.loc[df['_1순위신용체크구분'] == '신용', '_2순위신용체크구분'] = '체크'\n",
    "        df.loc[df['_1순위신용체크구분'] == '체크', '_2순위신용체크구분'] = '신용'\n",
    "        df.loc[df['_1순위신용체크구분'] == '미사용', '_2순위신용체크구분'] = '미사용'\n",
    "\n",
    "    # 7. 혜택수혜율 처리\n",
    "    if '혜택수혜율_R3M' in df.columns:\n",
    "        df['혜택수혜율_B0M'] = np.where(\n",
    "            df['혜택수혜율_B0M'].isna() & df['혜택수혜율_R3M'].notna(),\n",
    "            df['혜택수혜율_R3M'],\n",
    "            df['혜택수혜율_B0M']\n",
    "        )\n",
    "        df['혜택수혜율_B0M'] = df['혜택수혜율_B0M'].fillna(0)\n",
    "\n",
    "        df['혜택수혜율_R3M'] = np.where(\n",
    "            df['혜택수혜율_R3M'].isna() & df['혜택수혜율_B0M'].notna(),\n",
    "            df['혜택수혜율_B0M'],\n",
    "            df['혜택수혜율_R3M']\n",
    "        )\n",
    "        df['혜택수혜율_R3M'] = df['혜택수혜율_R3M'].fillna(0)\n",
    "\n",
    "        # 도메인 반영\n",
    "        if '소지여부_신용' in df.columns:\n",
    "            df.loc[(df['소지여부_신용'] == 'N') | (df['이용금액_R3M_신용'] == 0), ['혜택수혜율_B0M', '혜택수혜율_R3M']] = 0\n",
    "\n",
    "    # 8. 날짜 열 결측치 처리\n",
    "    date_cols = ['최종유효년월_신용_이용', '최종유효년월_신용_이용가능', '최종카드발급일자']\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(-1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 전처리 적용\n",
    "train_sampled_df = preprocess_df(train_sampled_df)\n",
    "evaluation_df = preprocess_df(evaluation_df)\n",
    "\n",
    "# 결과 확인 (선택적)\n",
    "print(\"train_sampled_df after preprocessing:\")\n",
    "print(train_sampled_df.head())\n",
    "print(\"evaluation_df after preprocessing:\")\n",
    "print(evaluation_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a761c34e-bae0-4351-beed-bba37bafa358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TabNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.93222 | test_accuracy: 0.77705 |  0:00:23s\n",
      "epoch 1  | loss: 0.57087 | test_accuracy: 0.42175 |  0:00:50s\n",
      "epoch 2  | loss: 0.51032 | test_accuracy: 0.76045 |  0:01:04s\n",
      "epoch 3  | loss: 0.46622 | test_accuracy: 0.7983  |  0:01:18s\n",
      "epoch 4  | loss: 0.4302  | test_accuracy: 0.77005 |  0:01:32s\n",
      "epoch 5  | loss: 0.42396 | test_accuracy: 0.80905 |  0:01:51s\n",
      "epoch 6  | loss: 0.40947 | test_accuracy: 0.8293  |  0:02:05s\n",
      "epoch 7  | loss: 0.39964 | test_accuracy: 0.8361  |  0:02:19s\n",
      "epoch 8  | loss: 0.39249 | test_accuracy: 0.8378  |  0:02:32s\n",
      "epoch 9  | loss: 0.39234 | test_accuracy: 0.84085 |  0:02:46s\n",
      "epoch 10 | loss: 0.39019 | test_accuracy: 0.83985 |  0:03:01s\n",
      "epoch 11 | loss: 0.38356 | test_accuracy: 0.8408  |  0:03:15s\n",
      "epoch 12 | loss: 0.3766  | test_accuracy: 0.8423  |  0:03:30s\n",
      "epoch 13 | loss: 0.37444 | test_accuracy: 0.8405  |  0:03:43s\n",
      "epoch 14 | loss: 0.37173 | test_accuracy: 0.83625 |  0:03:59s\n",
      "epoch 15 | loss: 0.36827 | test_accuracy: 0.84145 |  0:04:14s\n",
      "epoch 16 | loss: 0.36452 | test_accuracy: 0.8391  |  0:04:31s\n",
      "epoch 17 | loss: 0.36625 | test_accuracy: 0.84135 |  0:04:46s\n",
      "epoch 18 | loss: 0.368   | test_accuracy: 0.84465 |  0:05:01s\n",
      "epoch 19 | loss: 0.36425 | test_accuracy: 0.84205 |  0:05:16s\n",
      "epoch 20 | loss: 0.3624  | test_accuracy: 0.84095 |  0:05:31s\n",
      "epoch 21 | loss: 0.35924 | test_accuracy: 0.84445 |  0:05:47s\n",
      "epoch 22 | loss: 0.35612 | test_accuracy: 0.84295 |  0:06:03s\n",
      "epoch 23 | loss: 0.35343 | test_accuracy: 0.8419  |  0:06:19s\n",
      "epoch 24 | loss: 0.3504  | test_accuracy: 0.83785 |  0:06:33s\n",
      "epoch 25 | loss: 0.34639 | test_accuracy: 0.8404  |  0:06:49s\n",
      "epoch 26 | loss: 0.34824 | test_accuracy: 0.82205 |  0:07:06s\n",
      "epoch 27 | loss: 0.34763 | test_accuracy: 0.84285 |  0:07:22s\n",
      "epoch 28 | loss: 0.34708 | test_accuracy: 0.8409  |  0:07:37s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_test_accuracy = 0.84465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet - Train F1: 0.8345, Test F1: 0.8285\n"
     ]
    }
   ],
   "source": [
    "# 필수 라이브러리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "# 시드 고정 함수\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# 데이터셋 준비\n",
    "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]\n",
    "X = train_sampled_df[feature_cols].copy()\n",
    "y = train_sampled_df[\"Segment\"].copy()\n",
    "\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(y)\n",
    "\n",
    "X_test = evaluation_df[feature_cols].copy()\n",
    "y_true = evaluation_df[\"Segment\"].copy()\n",
    "y_true_encoded = le_target.transform(y_true)\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    X_test[col] = X_test[col].fillna('missing').astype(str)\n",
    "    unseen = set(X_test[col]) - set(le.classes_)\n",
    "    if unseen:\n",
    "        le.classes_ = np.append(le.classes_, list(unseen))\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# -------------------------------\n",
    "# TabNet 학습 및 평가\n",
    "# -------------------------------\n",
    "try:\n",
    "    print(\"Training TabNet...\")\n",
    "    clf_tabnet = TabNetClassifier(seed=42)\n",
    "    clf_tabnet.fit(\n",
    "        X.values, y_encoded,\n",
    "        eval_set=[(X_test.values, y_true_encoded)],\n",
    "        eval_name=['test'],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=100,\n",
    "        patience=10,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    y_pred_train_tabnet = clf_tabnet.predict(X.values)\n",
    "    y_pred_tabnet = clf_tabnet.predict(X_test.values)\n",
    "\n",
    "    f1_train_tabnet = f1_score(y_encoded, y_pred_train_tabnet, average='weighted')\n",
    "    f1_test_tabnet = f1_score(y_true_encoded, y_pred_tabnet, average='weighted')\n",
    "    print(f\"TabNet - Train F1: {f1_train_tabnet:.4f}, Test F1: {f1_test_tabnet:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"TabNet failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d97926d4-2a40-4a59-a4f4-90169ce3a62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Transformer 학습 시작...\n",
      "FT Transformer 실패: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 12364800000 bytes.\n"
     ]
    }
   ],
   "source": [
    "# 필수 라이브러리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import rtdl  # FT Transformer 라이브러리\n",
    "import torch.nn as nn\n",
    "\n",
    "# 시드 고정 함수\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# 데이터셋 준비\n",
    "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]  # 피처 열 선택\n",
    "X = train_sampled_df[feature_cols].copy()  # 훈련 데이터 복사\n",
    "y = train_sampled_df[\"Segment\"].copy()  # 타겟 데이터 복사\n",
    "\n",
    "X_test = evaluation_df[feature_cols].copy()  # 테스트 데이터 복사\n",
    "y_true = evaluation_df[\"Segment\"].copy()  # 테스트 타겟 복사\n",
    "\n",
    "# 타겟 인코딩 (타겟은 여전히 숫자로 변환 필요)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_target = LabelEncoder()  # 타겟 라벨 인코더\n",
    "y_encoded = le_target.fit_transform(y)  # 훈련 타겟 인코딩\n",
    "y_true_encoded = le_target.transform(y_true)  # 테스트 타겟 인코딩\n",
    "\n",
    "# 범주형 및 수치형 피처 분리\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()  # 범주형 피처 목록\n",
    "numerical_features = [col for col in feature_cols if col not in categorical_features]  # 수치형 피처 목록\n",
    "\n",
    "# 수치형 피처 스케일링\n",
    "scaler = StandardScaler()  # 표준화 스케일러\n",
    "X_num = scaler.fit_transform(X[numerical_features])  # 훈련 데이터 수치형 피처 스케일링\n",
    "X_test_num = scaler.transform(X_test[numerical_features])  # 테스트 데이터 수치형 피처 스케일링\n",
    "\n",
    "# 범주형 피처의 카디널리티 계산\n",
    "cat_cardinalities = [len(X[col].unique()) for col in categorical_features]  # 각 범주형 피처의 고유 값 개수\n",
    "\n",
    "# 범주형 피처를 정수형으로 변환 (0부터 시작하는 인덱스로)\n",
    "X_cat = np.stack([X[col].astype('category').cat.codes for col in categorical_features], axis=1)  # 훈련 데이터 범주형 변환\n",
    "X_test_cat = np.stack([X_test[col].fillna('missing').astype('category').cat.codes for col in categorical_features], axis=1)  # 테스트 데이터 범주형 변환\n",
    "\n",
    "# FT Transformer용 데이터 준비 (torch 텐서로 변환)\n",
    "X_num_tensor = torch.tensor(X_num, dtype=torch.float32)  # 훈련 수치형 데이터 텐서\n",
    "X_cat_tensor = torch.tensor(X_cat, dtype=torch.long) if categorical_features else None  # 훈련 범주형 데이터 텐서 (없으면 None)\n",
    "y_encoded_tensor = torch.tensor(y_encoded, dtype=torch.long)  # 훈련 타겟 텐서\n",
    "X_test_num_tensor = torch.tensor(X_test_num, dtype=torch.float32)  # 테스트 수치형 데이터 텐서\n",
    "X_test_cat_tensor = torch.tensor(X_test_cat, dtype=torch.long) if categorical_features else None  # 테스트 범주형 데이터 텐서\n",
    "y_true_encoded_tensor = torch.tensor(y_true_encoded, dtype=torch.long)  # 테스트 타겟 텐서\n",
    "\n",
    "# -------------------------------\n",
    "# FT Transformer 학습 및 평가\n",
    "# -------------------------------\n",
    "try:\n",
    "    print(\"FT Transformer 학습 시작...\")\n",
    "    \n",
    "    # FT Transformer 모델 정의\n",
    "    d_out = 64  # FT Transformer 출력 차원 (임의로 64로 설정, 조정 가능)\n",
    "    ft_transformer = rtdl.FTTransformer.make_default(\n",
    "        n_num_features=len(numerical_features),  # 수치형 피처 수\n",
    "        cat_cardinalities=cat_cardinalities if categorical_features else None,  # 범주형 피처 카디널리티\n",
    "        d_out=d_out  # 출력 차원 명시\n",
    "    )\n",
    "\n",
    "    # 분류를 위한 출력 레이어 정의\n",
    "    n_classes = len(np.unique(y_encoded))  # 클래스 수\n",
    "    classifier = nn.Linear(d_out, n_classes)  # 출력 레이어\n",
    "\n",
    "    # 옵티마이저와 손실 함수 정의\n",
    "    optimizer = torch.optim.Adam(list(ft_transformer.parameters()) + list(classifier.parameters()), lr=1e-3)  # Adam 옵티마이저\n",
    "    loss_fn = nn.CrossEntropyLoss()  # 교차 엔트로피 손실 함수\n",
    "\n",
    "    # 장치 설정\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU 사용 여부 확인\n",
    "    ft_transformer.to(device)  # FT Transformer를 장치로 이동\n",
    "    classifier.to(device)  # 출력 레이어를 장치로 이동\n",
    "    X_num_tensor = X_num_tensor.to(device)  # 훈련 수치형 데이터 장치로 이동\n",
    "    X_cat_tensor = X_cat_tensor.to(device) if X_cat_tensor is not None else None  # 훈련 범주형 데이터 장치로 이동\n",
    "    y_encoded_tensor = y_encoded_tensor.to(device)  # 훈련 타겟 장치로 이동\n",
    "    X_test_num_tensor = X_test_num_tensor.to(device)  # 테스트 수치형 데이터 장치로 이동\n",
    "    X_test_cat_tensor = X_test_cat_tensor.to(device) if X_test_cat_tensor is not None else None  # 테스트 범주형 데이터 장치로 이동\n",
    "    y_true_encoded_tensor = y_true_encoded_tensor.to(device)  # 테스트 타겟 장치로 이동\n",
    "\n",
    "    # 학습 루프\n",
    "    max_epochs = 100  # 최대 에포크 수\n",
    "    patience = 10  # 조기 종료 인내심\n",
    "    best_f1 = -float('inf')  # 최고 F1 점수 초기화\n",
    "    patience_counter = 0  # 조기 종료 카운터\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        ft_transformer.train()  # 학습 모드\n",
    "        classifier.train()\n",
    "        optimizer.zero_grad()  # 기울기 초기화\n",
    "        transformer_output = ft_transformer(X_num_tensor, X_cat_tensor)  # FT Transformer 출력\n",
    "        outputs = classifier(transformer_output)  # 출력 레이어로 클래스 예측\n",
    "        loss = loss_fn(outputs, y_encoded_tensor)  # 손실 계산\n",
    "        loss.backward()  # 역전파\n",
    "        optimizer.step()  # 가중치 업데이트\n",
    "\n",
    "        # 평가\n",
    "        ft_transformer.eval()  # 평가 모드\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():  # 기울기 계산 비활성화\n",
    "            test_transformer_output = ft_transformer(X_test_num_tensor, X_test_cat_tensor)  # 테스트 데이터로 FT Transformer 출력\n",
    "            y_pred_test = classifier(test_transformer_output).argmax(dim=1).cpu().numpy()  # 테스트 예측\n",
    "            f1_test = f1_score(y_true_encoded, y_pred_test, average='weighted')  # 테스트 F1 점수\n",
    "            print(f\"에포크 {epoch+1}/{max_epochs}, 테스트 F1: {f1_test:.4f}\")\n",
    "\n",
    "            # 조기 종료\n",
    "            if f1_test > best_f1:\n",
    "                best_f1 = f1_test  # 최고 F1 점수 갱신\n",
    "                patience_counter = 0  # 카운터 초기화\n",
    "            else:\n",
    "                patience_counter += 1  # 카운터 증가\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"조기 종료가 실행되었습니다.\")\n",
    "                    break\n",
    "\n",
    "    # 최종 예측\n",
    "    ft_transformer.eval()  # 평가 모드\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        train_transformer_output = ft_transformer(X_num_tensor, X_cat_tensor)\n",
    "        y_pred_train_ft = classifier(train_transformer_output).argmax(dim=1).cpu().numpy()  # 훈련 데이터 예측\n",
    "        test_transformer_output = ft_transformer(X_test_num_tensor, X_test_cat_tensor)\n",
    "        y_pred_test_ft = classifier(test_transformer_output).argmax(dim=1).cpu().numpy()  # 테스트 데이터 예측\n",
    "\n",
    "    f1_train_ft = f1_score(y_encoded, y_pred_train_ft, average='weighted')  # 훈련 F1 점수\n",
    "    f1_test_ft = f1_score(y_true_encoded, y_pred_test_ft, average='weighted')  # 테스트 F1 점수\n",
    "    print(f\"FT Transformer - 훈련 F1: {f1_train_ft:.4f}, 테스트 F1: {f1_test_ft:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"FT Transformer 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a64dcf1-acc4-4848-9a17-dbd211aa97ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Transformer 학습 시작...\n",
      "FT Transformer 실패: FTTransformer.make_default() got an unexpected keyword argument 'd_token'\n"
     ]
    }
   ],
   "source": [
    "# 필수 라이브러리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import rtdl  # FT Transformer 라이브러리\n",
    "import torch.nn as nn\n",
    "\n",
    "# 시드 고정 함수\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# 데이터셋 준비\n",
    "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]  # 피처 열 선택\n",
    "X = train_sampled_df[feature_cols].copy()  # 훈련 데이터 복사\n",
    "y = train_sampled_df[\"Segment\"].copy()  # 타겟 데이터 복사\n",
    "\n",
    "X_test = evaluation_df[feature_cols].copy()  # 테스트 데이터 복사\n",
    "y_true = evaluation_df[\"Segment\"].copy()  # 테스트 타겟 복사\n",
    "\n",
    "# 타겟 인코딩\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_target = LabelEncoder()  # 타겟 라벨 인코더\n",
    "y_encoded = le_target.fit_transform(y)  # 훈련 타겟 인코딩\n",
    "y_true_encoded = le_target.transform(y_true)  # 테스트 타겟 인코딩\n",
    "\n",
    "# 범주형 및 수치형 피처 분리\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()  # 범주형 피처 목록\n",
    "numerical_features = [col for col in feature_cols if col not in categorical_features]  # 수치형 피처 목록\n",
    "\n",
    "# 수치형 피처 스케일링\n",
    "scaler = StandardScaler()  # 표준화 스케일러\n",
    "X_num = scaler.fit_transform(X[numerical_features])  # 훈련 데이터 수치형 피처 스케일링\n",
    "X_test_num = scaler.transform(X_test[numerical_features])  # 테스트 데이터 수치형 피처 스케일링\n",
    "\n",
    "# 범주형 피처의 카디널리티 계산\n",
    "cat_cardinalities = [len(X[col].unique()) for col in categorical_features]  # 각 범주형 피처의 고유 값 개수\n",
    "\n",
    "# 범주형 피처를 정수형으로 변환\n",
    "X_cat = np.stack([X[col].astype('category').cat.codes for col in categorical_features], axis=1)  # 훈련 데이터 범주형 변환\n",
    "X_test_cat = np.stack([X_test[col].fillna('missing').astype('category').cat.codes for col in categorical_features], axis=1)  # 테스트 데이터 범주형 변환\n",
    "\n",
    "# FT Transformer용 데이터 준비 (torch 텐서로 변환)\n",
    "X_num_tensor = torch.tensor(X_num, dtype=torch.float32)  # 훈련 수치형 데이터 텐서\n",
    "X_cat_tensor = torch.tensor(X_cat, dtype=torch.long) if categorical_features else None  # 훈련 범주형 데이터 텐서\n",
    "y_encoded_tensor = torch.tensor(y_encoded, dtype=torch.long)  # 훈련 타겟 텐서\n",
    "X_test_num_tensor = torch.tensor(X_test_num, dtype=torch.float32)  # 테스트 수치형 데이터 텐서\n",
    "X_test_cat_tensor = torch.tensor(X_test_cat, dtype=torch.long) if categorical_features else None  # 테스트 범주형 데이터 텐서\n",
    "y_true_encoded_tensor = torch.tensor(y_true_encoded, dtype=torch.long)  # 테스트 타겟 텐서\n",
    "\n",
    "# DataLoader로 배치 처리 준비\n",
    "batch_size = 1024  # 배치 크기 (메모리에 따라 조정 가능)\n",
    "train_dataset = TensorDataset(X_num_tensor, X_cat_tensor if X_cat_tensor is not None else torch.zeros_like(X_num_tensor), y_encoded_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_num_tensor, X_test_cat_tensor if X_test_cat_tensor is not None else torch.zeros_like(X_test_num_tensor), y_true_encoded_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -------------------------------\n",
    "# FT Transformer 학습 및 평가\n",
    "# -------------------------------\n",
    "try:\n",
    "    print(\"FT Transformer 학습 시작...\")\n",
    "    \n",
    "    # FT Transformer 모델 정의\n",
    "    d_out = 32  # 출력 차원 줄임 (메모리 절약)\n",
    "    ft_transformer = rtdl.FTTransformer.make_default(\n",
    "        n_num_features=len(numerical_features),  # 수치형 피처 수\n",
    "        cat_cardinalities=cat_cardinalities if categorical_features else None,  # 범주형 피처 카디널리티\n",
    "        d_out=d_out,  # 출력 차원 명시\n",
    "        d_token=96,  # 토큰 차원 줄임 (기본값 192 -> 96)\n",
    "        n_layers=2   # 레이어 수 줄임 (기본값 3 -> 2)\n",
    "    )\n",
    "\n",
    "    # 분류를 위한 출력 레이어 정의\n",
    "    n_classes = len(np.unique(y_encoded))  # 클래스 수\n",
    "    classifier = nn.Linear(d_out, n_classes)  # 출력 레이어\n",
    "\n",
    "    # 장치 설정\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU 사용 여부 확인\n",
    "    ft_transformer.to(device)  # FT Transformer를 장치로 이동\n",
    "    classifier.to(device)  # 출력 레이어를 장치로 이동\n",
    "\n",
    "    # 옵티마이저와 손실 함수 정의\n",
    "    optimizer = torch.optim.Adam(list(ft_transformer.parameters()) + list(classifier.parameters()), lr=1e-3)  # Adam 옵티마이저\n",
    "    loss_fn = nn.CrossEntropyLoss()  # 교차 엔트로피 손실 함수\n",
    "\n",
    "    # 학습 루프\n",
    "    max_epochs = 100  # 최대 에포크 수\n",
    "    patience = 10  # 조기 종료 인내심\n",
    "    best_f1 = -float('inf')  # 최고 F1 점수 초기화\n",
    "    patience_counter = 0  # 조기 종료 카운터\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        ft_transformer.train()  # 학습 모드\n",
    "        classifier.train()\n",
    "        for batch in train_loader:\n",
    "            x_num_batch, x_cat_batch, y_batch = [b.to(device) for b in batch]\n",
    "            if X_cat_tensor is None:  # 범주형 데이터가 없는 경우\n",
    "                x_cat_batch = None\n",
    "            optimizer.zero_grad()  # 기울기 초기화\n",
    "            transformer_output = ft_transformer(x_num_batch, x_cat_batch)  # FT Transformer 출력\n",
    "            outputs = classifier(transformer_output)  # 출력 레이어로 클래스 예측\n",
    "            loss = loss_fn(outputs, y_batch)  # 손실 계산\n",
    "            loss.backward()  # 역전파\n",
    "            optimizer.step()  # 가중치 업데이트\n",
    "\n",
    "        # 평가\n",
    "        ft_transformer.eval()  # 평가 모드\n",
    "        classifier.eval()\n",
    "        y_pred_test_all = []\n",
    "        with torch.no_grad():  # 기울기 계산 비활성화\n",
    "            for batch in test_loader:\n",
    "                x_num_batch, x_cat_batch, y_batch = [b.to(device) for b in batch]\n",
    "                if X_test_cat_tensor is None:  # 범주형 데이터가 없는 경우\n",
    "                    x_cat_batch = None\n",
    "                test_transformer_output = ft_transformer(x_num_batch, x_cat_batch)  # 테스트 데이터로 FT Transformer 출력\n",
    "                y_pred_test = classifier(test_transformer_output).argmax(dim=1).cpu().numpy()  # 테스트 예측\n",
    "                y_pred_test_all.extend(y_pred_test)\n",
    "        \n",
    "        f1_test = f1_score(y_true_encoded, y_pred_test_all, average='weighted')  # 테스트 F1 점수\n",
    "        print(f\"에포크 {epoch+1}/{max_epochs}, 테스트 F1: {f1_test:.4f}\")\n",
    "\n",
    "        # 조기 종료\n",
    "        if f1_test > best_f1:\n",
    "            best_f1 = f1_test  # 최고 F1 점수 갱신\n",
    "            patience_counter = 0  # 카운터 초기화\n",
    "        else:\n",
    "            patience_counter += 1  # 카운터 증가\n",
    "            if patience_counter >= patience:\n",
    "                print(\"조기 종료가 실행되었습니다.\")\n",
    "                break\n",
    "\n",
    "    # 최종 예측 (훈련 데이터)\n",
    "    ft_transformer.eval()  # 평가 모드\n",
    "    classifier.eval()\n",
    "    y_pred_train_all = []\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            x_num_batch, x_cat_batch, y_batch = [b.to(device) for b in batch]\n",
    "            if X_cat_tensor is None:\n",
    "                x_cat_batch = None\n",
    "            train_transformer_output = ft_transformer(x_num_batch, x_cat_batch)\n",
    "            y_pred_train = classifier(train_transformer_output).argmax(dim=1).cpu().numpy()\n",
    "            y_pred_train_all.extend(y_pred_train)\n",
    "\n",
    "    f1_train_ft = f1_score(y_encoded, y_pred_train_all, average='weighted')  # 훈련 F1 점수\n",
    "    f1_test_ft = f1_score(y_true_encoded, y_pred_test_all, average='weighted')  # 테스트 F1 점수\n",
    "    print(f\"FT Transformer - 훈련 F1: {f1_train_ft:.4f}, 테스트 F1: {f1_test_ft:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"FT Transformer 실패: {e}\")\n",
    "\n",
    "# 메모리 정리\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
