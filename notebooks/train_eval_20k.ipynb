{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2400000개 데이터 E:1,922,052, D:349,242, C:127,590 ,A:972, B:144\n",
        "# A,B에서 오버 샘플링 진행하고 E,D,C 언더 샘플링\n",
        "# 1000,2000,5000개로 시도"
      ],
      "metadata": {
        "id": "EB804c9Umr6O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx91bwGZaszF"
      },
      "source": [
        "CatBoost돌릴거면 numpy 버전 pip하고 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터로딩"
      ],
      "metadata": {
        "id": "hhZT0-EKTQ3f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MFXFYk1oZRa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh8DI1muoij7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "43f9bfd8-32c9-4a9c-db79-7a4b0008139e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-accc53ad18a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/신용카드고객/train_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/신용카드고객/test_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_df=pd.read_csv('/content/drive/MyDrive/신용카드고객/train_df.csv')\n",
        "test_df=pd.read_csv('/content/drive/MyDrive/신용카드고객/test_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KprGEbJ3ol34"
      },
      "outputs": [],
      "source": [
        "# 원본 데이터프레임에서 20,000개 샘플링 (훈련 데이터)\n",
        "train_sampled_df = train_df.sample(n=20000, random_state=42)\n",
        "\n",
        "# 샘플링된 데이터 저장 (훈련 데이터)\n",
        "train_sampled_df.to_csv('/content/drive/MyDrive/신용카드고객/train_sampled_df.csv', index=False)\n",
        "\n",
        "# 뽑히지 않은 데이터 추출\n",
        "train_remaining_df = train_df[~train_df.index.isin(train_sampled_df.index)]\n",
        "\n",
        "# 남은 데이터에서 검증용 20,000개 샘플링\n",
        "evaluation_df = train_remaining_df.sample(n=20000, random_state=42)\n",
        "\n",
        "# 검증용 데이터 저장\n",
        "evaluation_df.to_csv('/content/drive/MyDrive/신용카드고객/evaluation_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noPg-tkwKdiX"
      },
      "outputs": [],
      "source": [
        "train_sampled_df=pd.read_csv('/content/drive/MyDrive/신용카드고객/train_sampled_df.csv')\n",
        "evaluation_df=pd.read_csv('/content/drive/MyDrive/신용카드고객/evaluation_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwJ3FQ53ECAn",
        "outputId": "dfd317b9-6790-4468-e8e1-0e58a0a57d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-82ab14d2a8ee>:4: DtypeWarning: Columns (385) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  evaluation_df=pd.read_csv('/content/drive/MyDrive/신용카드고객/evaluation_df.csv')\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_sampled_df=pd.read_csv('/content/drive/MyDrive/신용카드고객/train_sampled_df.csv')\n",
        "evaluation_df=pd.read_csv('/content/drive/MyDrive/신용카드고객/evaluation_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_sampled_df[\"Segment\"].value_counts())\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x=train_sampled_df[\"Segment\"])\n",
        "plt.title(\"Class Distribution in Segment\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "OndoGlHIBYzU",
        "outputId": "494f8019-8919-4ab9-f273-667f3bbb094d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segment\n",
            "E    15937\n",
            "D     2989\n",
            "C     1063\n",
            "A       10\n",
            "B        1\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluation_df[\"Segment\"].value_counts())\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x=evaluation_df[\"Segment\"])\n",
        "plt.title(\"Class Distribution in Segment\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "v07a1vauBZkK",
        "outputId": "bf9eb51e-5a87-46c6-fdb0-d8c27629bcc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segment\n",
            "E    15877\n",
            "D     3009\n",
            "C     1103\n",
            "A       11\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rci8i9C0wby0"
      },
      "source": [
        "# 전처리 코드 버전 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTxoEwZJKynW",
        "outputId": "13f70f84-5523-4782-b2a1-fc098893dff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_sampled_df after preprocessing:\n",
            "   Unnamed: 0    기준년월            ID  남녀구분코드   연령 Segment  회원여부_이용가능  \\\n",
            "0      599866  201808  TRAIN_199866       2  30대       E          1   \n",
            "1      427635  201808  TRAIN_027635       1  50대       E          0   \n",
            "2     1833852  201811  TRAIN_233852       2  40대       E          1   \n",
            "3      690821  201808  TRAIN_290821       1  40대       E          1   \n",
            "4     1098632  201809  TRAIN_298632       2  30대       D          1   \n",
            "\n",
            "   회원여부_이용가능_CA  회원여부_이용가능_카드론  소지여부_신용  ...  변동률_RV일시불평잔  변동률_할부평잔  변동률_CA평잔  \\\n",
            "0             1              0        1  ...     0.999998  1.987409  0.999998   \n",
            "1             0              0        1  ...     0.999998  0.999998  0.999998   \n",
            "2             1              1        1  ...     0.999998  0.999998  0.999998   \n",
            "3             1              1        1  ...     0.999998  0.904525  0.999998   \n",
            "4             1              1        1  ...     0.952195  0.604032  0.999998   \n",
            "\n",
            "   변동률_RVCA평잔  변동률_카드론평잔  변동률_잔액_B1M  변동률_잔액_일시불_B1M  변동률_잔액_CA_B1M  \\\n",
            "0    0.999998   0.999998    0.147471       -0.116887            0.0   \n",
            "1    0.999998   0.999998    0.000000        0.000000            0.0   \n",
            "2    0.999998   0.999998    0.000000        0.000000            0.0   \n",
            "3    0.999998   0.999998   -0.025178        0.102291            0.0   \n",
            "4    0.999998   0.999998    0.199584        0.085968            0.0   \n",
            "\n",
            "   혜택수혜율_R3M  혜택수혜율_B0M  \n",
            "0   0.000000   0.000000  \n",
            "1   0.000000   0.000000  \n",
            "2   0.000000   0.000000  \n",
            "3   3.111331   3.220928  \n",
            "4  -0.084577   0.075305  \n",
            "\n",
            "[5 rows x 853 columns]\n",
            "evaluation_df after preprocessing:\n",
            "   Unnamed: 0    기준년월            ID  남녀구분코드   연령 Segment  회원여부_이용가능  \\\n",
            "0      140607  201807  TRAIN_140607       1  60대       E          1   \n",
            "1      615413  201808  TRAIN_215413       1  50대       E          1   \n",
            "2     2128921  201812  TRAIN_128921       1  50대       E          1   \n",
            "3      494497  201808  TRAIN_094497       2  40대       D          1   \n",
            "4     1814277  201811  TRAIN_214277       2  30대       E          1   \n",
            "\n",
            "   회원여부_이용가능_CA  회원여부_이용가능_카드론  소지여부_신용  ...  변동률_RV일시불평잔  변동률_할부평잔  변동률_CA평잔  \\\n",
            "0             1              1        1  ...     0.999998  0.999998  0.999998   \n",
            "1             1              0        1  ...     0.999998  0.901223  0.999998   \n",
            "2             1              1        1  ...     0.999998  0.999998  0.999998   \n",
            "3             1              0        1  ...     0.999998  1.990905  1.002733   \n",
            "4             1              1        1  ...     0.999998  0.000000  0.999998   \n",
            "\n",
            "   변동률_RVCA평잔  변동률_카드론평잔  변동률_잔액_B1M  변동률_잔액_일시불_B1M  변동률_잔액_CA_B1M  \\\n",
            "0    0.999998   0.999998    0.000000        0.000000        0.00000   \n",
            "1    0.999998   0.999998   -0.009084       -0.050106        0.00000   \n",
            "2    0.999998   0.999998    0.000000        0.000000        0.00000   \n",
            "3    0.999998   1.453043   -0.153078        0.000000       -0.07013   \n",
            "4    0.999998   0.999998   -0.025110       -0.140781        0.00000   \n",
            "\n",
            "   혜택수혜율_R3M  혜택수혜율_B0M  \n",
            "0   0.000000   0.000000  \n",
            "1   4.216123   4.244498  \n",
            "2   0.000000   0.000000  \n",
            "3   0.000000   0.000000  \n",
            "4   1.732865   2.974604  \n",
            "\n",
            "[5 rows x 853 columns]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def preprocess_df(df):\n",
        "    \"\"\"\n",
        "    train_sampled_df와 evaluation_df에 동일한 전처리를 적용하는 함수\n",
        "    \"\"\"\n",
        "    # 1. 업종 목록 결측치 처리\n",
        "    industry_list = [\n",
        "        '_3순위여유업종', '_3순위납부업종', '_2순위여유업종', '_3순위교통업종', '_2순위납부업종',\n",
        "        '_1순위여유업종', '_2순위교통업종', '_3순위쇼핑업종', '_1순위납부업종', '_1순위교통업종',\n",
        "        '_2순위쇼핑업종', '_3순위업종', '_1순위쇼핑업종', '_2순위업종', '_1순위업종'\n",
        "    ]\n",
        "    for industry in industry_list:\n",
        "        if industry in df.columns:\n",
        "            df[industry] = df[industry].fillna('없음')\n",
        "\n",
        "    # 2. 불필요한 열 삭제\n",
        "    columns_to_drop = [\n",
        "        '연체일자_B0M', '최종카드론_대출일자', '최종카드론_신청경로코드', '최종카드론_금융상환방식코드',\n",
        "        'RV신청일자', 'OS구분코드'\n",
        "    ]\n",
        "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
        "\n",
        "    # 3. 가입통신회사코드 처리\n",
        "    if '회원여부_이용가능' in df.columns and '이용금액_R3M_신용' in df.columns:\n",
        "        df['가입통신회사코드'] = np.where(\n",
        "            (df['회원여부_이용가능'] == 'N') | (df['이용금액_R3M_신용'] == 0),\n",
        "            '미가입',\n",
        "            df['가입통신회사코드'].fillna('Unknown')\n",
        "        )\n",
        "\n",
        "    # 4. 직장시도명 처리\n",
        "    if '거주시도명' in df.columns:\n",
        "        df['직장시도명'] = np.where(\n",
        "            df['직장시도명'].isna() & df['거주시도명'].notna(),\n",
        "            df['거주시도명'],\n",
        "            df['직장시도명'].fillna('Unknown')\n",
        "        )\n",
        "\n",
        "    # 5. RV전환가능여부 처리\n",
        "    if '소지여부_신용' in df.columns and '이용금액_R3M_신용' in df.columns:\n",
        "        df['RV전환가능여부'] = np.where(\n",
        "            (df['이용금액_R3M_신용'] == 0) | (df['소지여부_신용'] == 'N'),\n",
        "            'N',\n",
        "            df['RV전환가능여부'].fillna('Unknown')\n",
        "        )\n",
        "\n",
        "    # 6. _1순위신용체크구분 처리\n",
        "    if '_1순위업종' in df.columns and '이용금액_R3M_신용' in df.columns:\n",
        "        df['_1순위신용체크구분'] = np.where(\n",
        "            df['이용금액_R3M_신용'] == 0,\n",
        "            '미사용',\n",
        "            np.where(\n",
        "                df['_1순위신용체크구분'].isna() & df['_1순위업종'].notna() & (df['_1순위업종'] != '없음'),\n",
        "                '신용',\n",
        "                df['_1순위신용체크구분'].fillna('미사용')\n",
        "            )\n",
        "        )\n",
        "        # _1순위와 _2순위 상호작용\n",
        "        df.loc[df['_1순위신용체크구분'] == '신용', '_2순위신용체크구분'] = '체크'\n",
        "        df.loc[df['_1순위신용체크구분'] == '체크', '_2순위신용체크구분'] = '신용'\n",
        "        df.loc[df['_1순위신용체크구분'] == '미사용', '_2순위신용체크구분'] = '미사용'\n",
        "\n",
        "    # 7. 혜택수혜율 처리\n",
        "    if '혜택수혜율_R3M' in df.columns:\n",
        "        df['혜택수혜율_B0M'] = np.where(\n",
        "            df['혜택수혜율_B0M'].isna() & df['혜택수혜율_R3M'].notna(),\n",
        "            df['혜택수혜율_R3M'],\n",
        "            df['혜택수혜율_B0M']\n",
        "        )\n",
        "        df['혜택수혜율_B0M'] = df['혜택수혜율_B0M'].fillna(0)\n",
        "\n",
        "        df['혜택수혜율_R3M'] = np.where(\n",
        "            df['혜택수혜율_R3M'].isna() & df['혜택수혜율_B0M'].notna(),\n",
        "            df['혜택수혜율_B0M'],\n",
        "            df['혜택수혜율_R3M']\n",
        "        )\n",
        "        df['혜택수혜율_R3M'] = df['혜택수혜율_R3M'].fillna(0)\n",
        "\n",
        "        # 도메인 반영\n",
        "        if '소지여부_신용' in df.columns:\n",
        "            df.loc[(df['소지여부_신용'] == 'N') | (df['이용금액_R3M_신용'] == 0), ['혜택수혜율_B0M', '혜택수혜율_R3M']] = 0\n",
        "\n",
        "    # 8. 날짜 열 결측치 처리\n",
        "    date_cols = ['최종유효년월_신용_이용', '최종유효년월_신용_이용가능', '최종카드발급일자']\n",
        "    for col in date_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(-1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 전처리 적용\n",
        "train_sampled_df = preprocess_df(train_sampled_df)\n",
        "evaluation_df = preprocess_df(evaluation_df)\n",
        "\n",
        "# 결과 확인 (선택적)\n",
        "print(\"train_sampled_df after preprocessing:\")\n",
        "print(train_sampled_df.head())\n",
        "print(\"evaluation_df after preprocessing:\")\n",
        "print(evaluation_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리 코드 버전2"
      ],
      "metadata": {
        "id": "HDVK7jDjc9Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리시, 혜택수혜율 결측치 예측 및 대체를 위한 함수 작성 코드\n",
        "\n",
        "import pandas as pdf\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def predict_and_fill_missing_targets(\n",
        "    df: pd.DataFrame,\n",
        "    features: list,\n",
        "    targets: list,\n",
        "    sample_size: int = None,\n",
        "    model=None,\n",
        "    random_state: int = 42\n",
        "    ):\n",
        "    \"\"\"\n",
        "    멀티타겟 회귀를 이용해 결측값을 예측하고 원본 데이터프레임에 채워 넣는 함수\n",
        "\n",
        "    Parameters:\n",
        "    - df: 입력 데이터프레임\n",
        "    - features: 예측에 사용할 feature 컬럼 리스트\n",
        "    - targets: 결측치를 대체할 target 컬럼 리스트\n",
        "    - sample_size: 학습 및 예측에 사용할 샘플 수 (None이면 전체 사용)\n",
        "    - model: 사용할 회귀 모델 (None이면 RandomForest 기본 모델 사용)\n",
        "    - random_state: 샘플링 및 모델 재현성을 위한 시드값\n",
        "\n",
        "    Returns:\n",
        "    - df: 결측치가 대체된 새로운 데이터프레임\n",
        "    - before_preds: 결측치 이전의 원본 (NaN) 값들\n",
        "    - after_preds: 예측된 값들\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # 범주형 컬럼 인코딩\n",
        "    for col in features:\n",
        "        if df[col].dtype == 'object':\n",
        "            le = LabelEncoder()\n",
        "            df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "    # 학습/예측 데이터 분리\n",
        "    train = df[df[targets].notnull().all(axis=1)]\n",
        "    test = df[df[targets].isnull().any(axis=1)]\n",
        "\n",
        "    X_train, y_train = train[features], train[targets]\n",
        "    X_test = test[features]\n",
        "\n",
        "    # 기본 모델 설정\n",
        "    if model is None:\n",
        "        model = MultiOutputRegressor(RandomForestRegressor(n_estimators=50, random_state=random_state))\n",
        "\n",
        "    # 학습 및 예측\n",
        "    model.fit(X_train, y_train)\n",
        "    predicted = model.predict(X_test)\n",
        "\n",
        "    # 결과 반영\n",
        "    df.loc[test.index, targets] = predicted\n",
        "\n",
        "    # 예측 전/후 값 반환\n",
        "    before_preds = test[targets]\n",
        "    after_preds = pd.DataFrame(predicted, index=test.index, columns=targets)\n",
        "\n",
        "    return df, before_preds, after_preds"
      ],
      "metadata": {
        "id": "Trk71jPvYcSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수 - 1\n",
        "\n",
        "def preprocess_df(df):\n",
        "\n",
        "    # STEP_1 : 동일 값 컬럼 제거 (컬럼 내에 데이터가 모두 동일값이어서 분류모델에서 의미가 없음)\n",
        "\n",
        "    # 동일한 값만 있는 컬럼 리스트\n",
        "    # constant_cols 이중 리스트 → 평탄화\n",
        "    constant_cols = ['이용카드수_체크_가족', '이용금액_R3M_체크_가족', '연회비할인카드수_B0M', '할인금액_기본연회비_B0M', '할인금액_제휴연회비_B0M',\n",
        "                    '상품관련면제카드수_B0M', '임직원면제카드수_B0M', '우수회원면제카드수_B0M', '기타면제카드수_B0M', '시장연체상환여부_R3M',\n",
        "                    '이용건수_부분무이자_B0M', '이용금액_부분무이자_B0M', '여유_여행이용금액', '납부_렌탈료이용금액', '납부_유선방송이용금액',\n",
        "                    '납부_건강연금이용금액', '할부건수_부분_3M_R12M', '할부건수_부분_6M_R12M', '할부건수_부분_14M_R12M', '할부금액_부분_3M_R12M',\n",
        "                    'RP건수_유선방송_B0M', 'RP건수_건강_B0M', 'RP후경과월_유선방송', 'RP후경과월_건강', '증감_RP건수_유선방송_전월', '증감_RP건수_건강_전월',\n",
        "                    '이용개월수_당사페이_R6M', '이용금액_당사페이_R6M', '이용금액_당사기타_R6M', '이용건수_당사페이_R6M', '이용건수_당사기타_R6M',\n",
        "                    '이용금액_당사페이_R3M', '이용금액_당사기타_R3M', '이용건수_당사페이_R3M', '이용건수_당사기타_R3M', '이용금액_당사페이_B0M', '이용금액_당사기타_B0M',\n",
        "                    '이용건수_당사페이_B0M', '이용건수_당사기타_B0M', '승인거절건수_입력오류_B0M', '승인거절건수_기타_B0M', '대표결제방법코드', '카드론잔액_최종경과월',\n",
        "                    '최종연체개월수_R15M', 'RV잔액이월횟수_R6M', 'RV잔액이월횟수_R3M', '연체잔액_일시불_해외_B0M', '연체잔액_RV일시불_해외_B0M', '연체잔액_할부_해외_B0M',\n",
        "                    '연체잔액_CA_해외_B0M', '인입횟수_금융_IB_R6M', '인입불만횟수_IB_R6M', '인입불만일수_IB_R6M', '인입불만월수_IB_R6M', '인입불만후경과월_IB_R6M',\n",
        "                    '인입불만횟수_IB_B0M', '인입불만일수_IB_B0M', 'IB문의건수_한도_B0M', 'IB문의건수_결제_B0M', 'IB문의건수_할부_B0M', 'IB문의건수_정보변경_B0M',\n",
        "                    'IB문의건수_결제일변경_B0M', 'IB문의건수_명세서_B0M', 'IB문의건수_비밀번호_B0M', 'IB문의건수_SMS_B0M', 'IB문의건수_APP_B0M', 'IB문의건수_부대서비스_B0M',\n",
        "                    'IB문의건수_포인트_B0M', 'IB문의건수_BL_B0M', 'IB문의건수_분실도난_B0M', 'IB문의건수_CA_B0M', 'IB상담건수_VOC_B0M', 'IB상담건수_VOC민원_B0M',\n",
        "                    'IB상담건수_VOC불만_B0M', 'IB상담건수_금감원_B0M', 'IB문의건수_명세서_R6M', 'IB문의건수_APP_R6M', 'IB상담건수_VOC_R6M', 'IB상담건수_VOC민원_R6M',\n",
        "                    'IB상담건수_VOC불만_R6M', 'IB상담건수_금감원_R6M', '불만제기건수_B0M', '불만제기건수_R12M', '당사PAY_방문횟수_B0M', '당사PAY_방문횟수_R6M',\n",
        "                    '당사PAY_방문월수_R6M', '컨택건수_CA_TM_B0M', '컨택건수_포인트소진_TM_B0M', '컨택건수_CA_EM_B0M', '컨택건수_리볼빙_EM_B0M', '컨택건수_리볼빙_청구서_B0M',\n",
        "                    '컨택건수_카드론_인터넷_B0M', '컨택건수_CA_인터넷_B0M', '컨택건수_리볼빙_인터넷_B0M', '컨택건수_카드론_당사앱_B0M', '컨택건수_CA_당사앱_B0M',\n",
        "                    '컨택건수_리볼빙_당사앱_B0M', '컨택건수_CA_EM_R6M', '컨택건수_리볼빙_EM_R6M', '컨택건수_리볼빙_청구서_R6M', '컨택건수_카드론_인터넷_R6M',\n",
        "                    '컨택건수_CA_인터넷_R6M', '컨택건수_리볼빙_인터넷_R6M', '컨택건수_카드론_당사앱_R6M', '컨택건수_CA_당사앱_R6M', '컨택건수_리볼빙_당사앱_R6M',\n",
        "                    '컨택건수_FDS_B0M', '컨택건수_FDS_R6M']\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"동일한 값만 있는 컬럼 개수: {len(constant_cols)}\")\n",
        "    print(\"컬럼 이름 목록:\", constant_cols)\n",
        "\n",
        "    # 해당 컬럼 삭제\n",
        "    df = df.drop(columns=constant_cols)\n",
        "\n",
        "\n",
        "    print('------------------------------------<STEP_1 완료>------------------------------------')\n",
        "\n",
        "    # STEP_2 : 결측치 확인\n",
        "\n",
        "    # 3) 업종 목록 결측치 'Unknown' 처리 (➡️ 나중에 디벨롭 필요)\n",
        "    industry_list = [\n",
        "        '_3순위여유업종', '_3순위납부업종', '_2순위여유업종', '_3순위교통업종', '_2순위납부업종',\n",
        "        '_1순위여유업종', '_2순위교통업종', '_3순위쇼핑업종', '_1순위납부업종', '_1순위교통업종',\n",
        "        '_2순위쇼핑업종', '_3순위업종', '_1순위쇼핑업종', '_2순위업종', '_1순위업종'\n",
        "    ]\n",
        "    for industry in industry_list:\n",
        "        if industry in df.columns:\n",
        "            df[industry] = df[industry].fillna('없음')\n",
        "\n",
        "    # STEP_3 : 결측치 제거 및 대체\n",
        "\n",
        "    # 1) 결측률 80% 이상인 컬럼 제거\n",
        "\n",
        "    columns_to_drop = [\n",
        "        '연체일자_B0M', '최종카드론_대출일자', '최종카드론_신청경로코드', '최종카드론_금융상환방식코드',\n",
        "        'RV신청일자', 'OS구분코드'\n",
        "    ]\n",
        "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
        "\n",
        "    # 2) 기존 컬럼과 동일한 값을 포함하는 결측치 컬럼 제거 (직장도시명)\n",
        "    df = df.drop(columns=[\"직장도시명\"], errors='ignore')\n",
        "\n",
        "    # 4) 가입통신회사코드 결측치 처리\n",
        "    if '가입통신회사코드' in df.columns :\n",
        "        df['가입통신회사코드'] = np.where(\n",
        "            (df['회원여부_이용가능'] == 'N') | (df['이용금액_R3M_신용'] == 0),\n",
        "            '미가입',\n",
        "            df['가입통신회사코드'].fillna('Unknown')\n",
        "        )\n",
        "\n",
        "    # 5) 1순위신용체크구분, 2순위신용체크구분 'Unknown' 처리 (➡️ 나중에 디벨롭 필요)\n",
        "    df.loc[df['_1순위신용체크구분'] == '신용', '_2순위신용체크구분'] = '체크'\n",
        "    df.loc[df['_1순위신용체크구분'] == '체크', '_2순위신용체크구분'] = '신용'\n",
        "    df.loc[df['_1순위신용체크구분'] == '미사용', '_2순위신용체크구분'] = '미사용'\n",
        "\n",
        "\n",
        "    # 6) OS구분코드 'Unknown' 처리 (➡️ 나중에 디벨롭 필요)\n",
        "    columns_list = [ \"OS구분코드\"]\n",
        "    for industry in columns_list:\n",
        "        if industry in df.columns:\n",
        "            df[industry] = df[industry].fillna('Unknown')\n",
        "\n",
        "\n",
        "    # 7) 혜택수혜율_R3M, 혜택수혜율_B0M 결측치 예측 및 대체 -> 멀티타겟, 다중선형회귀분석\n",
        "    features = [\n",
        "    '이용금액_R3M_신용', '이용금액_R3M_체크',\n",
        "    '소지카드수_유효_신용', '유효카드수_체크',\n",
        "    '증감율_이용건수_일시불_전월', '이용건수_신판_R3M',\n",
        "    '연령', '남녀구분코드'\n",
        "    ]\n",
        "\n",
        "    targets = ['혜택수혜율_R3M', '혜택수혜율_B0M']\n",
        "\n",
        "    df, before_preds, after_preds = predict_and_fill_missing_targets(\n",
        "        df= df,\n",
        "        features= features,\n",
        "        targets= targets,\n",
        "        sample_size= 2000000\n",
        "        )\n",
        "\n",
        "    # 8) 최종카드발급일자 -> 결측치는 -1처리\n",
        "    df[\"최종카드발급일자\"] = df[\"최종카드발급일자\"].fillna(-1)\n",
        "\n",
        "    # 9) 최종유효년월_신용_이용, 최종유효년월_신용_이용가능 -> 최종 카드발급일자에 500(5년) 더하기\n",
        "    df[\"최종유효년월_신용_이용\"] = df[\"최종유효년월_신용_이용\"].fillna(-1)\n",
        "    df[\"최종유효년월_신용_이용가능\"] = df[\"최종유효년월_신용_이용가능\"].fillna(-1)\n",
        "\n",
        "    '''columns_list = [\"최종유효년월_신용_이용\", \"최종유효년월_신용_이용가능\"]\n",
        "    for industry in columns_list:\n",
        "        if industry in df.columns:\n",
        "            if df[\"최종카드발급일자\"] == -1 :\n",
        "                df[industry] = df[industry].fillna(-1)\n",
        "            else :\n",
        "                df[industry] = df[industry].fillna(500 + df['최종카드발급일자'].astype(str).str[:6].astype(int))'''\n",
        "\n",
        "    # 10) RV전환가능여부 -> 결측치 얼마 없어서 일단 Unknown 처리\n",
        "    columns_list = [\"RV전환가능여부\"]\n",
        "    for industry in columns_list:\n",
        "        if industry in df.columns:\n",
        "            df[industry] = df[industry].fillna('Unknown')\n",
        "\n",
        "    print('------------------------------------<STEP_3 완료>------------------------------------')\n",
        "\n",
        "    return df\n",
        "\n",
        "train_sampled_df = preprocess_df(train_sampled_df)\n",
        "evaluation_df = preprocess_df(evaluation_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqd7AGWGYhxR",
        "outputId": "c0666bf5-b617-4bc4-f40e-b9e1d8f841b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "동일한 값만 있는 컬럼 개수: 108\n",
            "컬럼 이름 목록: ['이용카드수_체크_가족', '이용금액_R3M_체크_가족', '연회비할인카드수_B0M', '할인금액_기본연회비_B0M', '할인금액_제휴연회비_B0M', '상품관련면제카드수_B0M', '임직원면제카드수_B0M', '우수회원면제카드수_B0M', '기타면제카드수_B0M', '시장연체상환여부_R3M', '이용건수_부분무이자_B0M', '이용금액_부분무이자_B0M', '여유_여행이용금액', '납부_렌탈료이용금액', '납부_유선방송이용금액', '납부_건강연금이용금액', '할부건수_부분_3M_R12M', '할부건수_부분_6M_R12M', '할부건수_부분_14M_R12M', '할부금액_부분_3M_R12M', 'RP건수_유선방송_B0M', 'RP건수_건강_B0M', 'RP후경과월_유선방송', 'RP후경과월_건강', '증감_RP건수_유선방송_전월', '증감_RP건수_건강_전월', '이용개월수_당사페이_R6M', '이용금액_당사페이_R6M', '이용금액_당사기타_R6M', '이용건수_당사페이_R6M', '이용건수_당사기타_R6M', '이용금액_당사페이_R3M', '이용금액_당사기타_R3M', '이용건수_당사페이_R3M', '이용건수_당사기타_R3M', '이용금액_당사페이_B0M', '이용금액_당사기타_B0M', '이용건수_당사페이_B0M', '이용건수_당사기타_B0M', '승인거절건수_입력오류_B0M', '승인거절건수_기타_B0M', '대표결제방법코드', '카드론잔액_최종경과월', '최종연체개월수_R15M', 'RV잔액이월횟수_R6M', 'RV잔액이월횟수_R3M', '연체잔액_일시불_해외_B0M', '연체잔액_RV일시불_해외_B0M', '연체잔액_할부_해외_B0M', '연체잔액_CA_해외_B0M', '인입횟수_금융_IB_R6M', '인입불만횟수_IB_R6M', '인입불만일수_IB_R6M', '인입불만월수_IB_R6M', '인입불만후경과월_IB_R6M', '인입불만횟수_IB_B0M', '인입불만일수_IB_B0M', 'IB문의건수_한도_B0M', 'IB문의건수_결제_B0M', 'IB문의건수_할부_B0M', 'IB문의건수_정보변경_B0M', 'IB문의건수_결제일변경_B0M', 'IB문의건수_명세서_B0M', 'IB문의건수_비밀번호_B0M', 'IB문의건수_SMS_B0M', 'IB문의건수_APP_B0M', 'IB문의건수_부대서비스_B0M', 'IB문의건수_포인트_B0M', 'IB문의건수_BL_B0M', 'IB문의건수_분실도난_B0M', 'IB문의건수_CA_B0M', 'IB상담건수_VOC_B0M', 'IB상담건수_VOC민원_B0M', 'IB상담건수_VOC불만_B0M', 'IB상담건수_금감원_B0M', 'IB문의건수_명세서_R6M', 'IB문의건수_APP_R6M', 'IB상담건수_VOC_R6M', 'IB상담건수_VOC민원_R6M', 'IB상담건수_VOC불만_R6M', 'IB상담건수_금감원_R6M', '불만제기건수_B0M', '불만제기건수_R12M', '당사PAY_방문횟수_B0M', '당사PAY_방문횟수_R6M', '당사PAY_방문월수_R6M', '컨택건수_CA_TM_B0M', '컨택건수_포인트소진_TM_B0M', '컨택건수_CA_EM_B0M', '컨택건수_리볼빙_EM_B0M', '컨택건수_리볼빙_청구서_B0M', '컨택건수_카드론_인터넷_B0M', '컨택건수_CA_인터넷_B0M', '컨택건수_리볼빙_인터넷_B0M', '컨택건수_카드론_당사앱_B0M', '컨택건수_CA_당사앱_B0M', '컨택건수_리볼빙_당사앱_B0M', '컨택건수_CA_EM_R6M', '컨택건수_리볼빙_EM_R6M', '컨택건수_리볼빙_청구서_R6M', '컨택건수_카드론_인터넷_R6M', '컨택건수_CA_인터넷_R6M', '컨택건수_리볼빙_인터넷_R6M', '컨택건수_카드론_당사앱_R6M', '컨택건수_CA_당사앱_R6M', '컨택건수_리볼빙_당사앱_R6M', '컨택건수_FDS_B0M', '컨택건수_FDS_R6M']\n",
            "------------------------------------<STEP_1 완료>------------------------------------\n",
            "------------------------------------<STEP_3 완료>------------------------------------\n",
            "동일한 값만 있는 컬럼 개수: 108\n",
            "컬럼 이름 목록: ['이용카드수_체크_가족', '이용금액_R3M_체크_가족', '연회비할인카드수_B0M', '할인금액_기본연회비_B0M', '할인금액_제휴연회비_B0M', '상품관련면제카드수_B0M', '임직원면제카드수_B0M', '우수회원면제카드수_B0M', '기타면제카드수_B0M', '시장연체상환여부_R3M', '이용건수_부분무이자_B0M', '이용금액_부분무이자_B0M', '여유_여행이용금액', '납부_렌탈료이용금액', '납부_유선방송이용금액', '납부_건강연금이용금액', '할부건수_부분_3M_R12M', '할부건수_부분_6M_R12M', '할부건수_부분_14M_R12M', '할부금액_부분_3M_R12M', 'RP건수_유선방송_B0M', 'RP건수_건강_B0M', 'RP후경과월_유선방송', 'RP후경과월_건강', '증감_RP건수_유선방송_전월', '증감_RP건수_건강_전월', '이용개월수_당사페이_R6M', '이용금액_당사페이_R6M', '이용금액_당사기타_R6M', '이용건수_당사페이_R6M', '이용건수_당사기타_R6M', '이용금액_당사페이_R3M', '이용금액_당사기타_R3M', '이용건수_당사페이_R3M', '이용건수_당사기타_R3M', '이용금액_당사페이_B0M', '이용금액_당사기타_B0M', '이용건수_당사페이_B0M', '이용건수_당사기타_B0M', '승인거절건수_입력오류_B0M', '승인거절건수_기타_B0M', '대표결제방법코드', '카드론잔액_최종경과월', '최종연체개월수_R15M', 'RV잔액이월횟수_R6M', 'RV잔액이월횟수_R3M', '연체잔액_일시불_해외_B0M', '연체잔액_RV일시불_해외_B0M', '연체잔액_할부_해외_B0M', '연체잔액_CA_해외_B0M', '인입횟수_금융_IB_R6M', '인입불만횟수_IB_R6M', '인입불만일수_IB_R6M', '인입불만월수_IB_R6M', '인입불만후경과월_IB_R6M', '인입불만횟수_IB_B0M', '인입불만일수_IB_B0M', 'IB문의건수_한도_B0M', 'IB문의건수_결제_B0M', 'IB문의건수_할부_B0M', 'IB문의건수_정보변경_B0M', 'IB문의건수_결제일변경_B0M', 'IB문의건수_명세서_B0M', 'IB문의건수_비밀번호_B0M', 'IB문의건수_SMS_B0M', 'IB문의건수_APP_B0M', 'IB문의건수_부대서비스_B0M', 'IB문의건수_포인트_B0M', 'IB문의건수_BL_B0M', 'IB문의건수_분실도난_B0M', 'IB문의건수_CA_B0M', 'IB상담건수_VOC_B0M', 'IB상담건수_VOC민원_B0M', 'IB상담건수_VOC불만_B0M', 'IB상담건수_금감원_B0M', 'IB문의건수_명세서_R6M', 'IB문의건수_APP_R6M', 'IB상담건수_VOC_R6M', 'IB상담건수_VOC민원_R6M', 'IB상담건수_VOC불만_R6M', 'IB상담건수_금감원_R6M', '불만제기건수_B0M', '불만제기건수_R12M', '당사PAY_방문횟수_B0M', '당사PAY_방문횟수_R6M', '당사PAY_방문월수_R6M', '컨택건수_CA_TM_B0M', '컨택건수_포인트소진_TM_B0M', '컨택건수_CA_EM_B0M', '컨택건수_리볼빙_EM_B0M', '컨택건수_리볼빙_청구서_B0M', '컨택건수_카드론_인터넷_B0M', '컨택건수_CA_인터넷_B0M', '컨택건수_리볼빙_인터넷_B0M', '컨택건수_카드론_당사앱_B0M', '컨택건수_CA_당사앱_B0M', '컨택건수_리볼빙_당사앱_B0M', '컨택건수_CA_EM_R6M', '컨택건수_리볼빙_EM_R6M', '컨택건수_리볼빙_청구서_R6M', '컨택건수_카드론_인터넷_R6M', '컨택건수_CA_인터넷_R6M', '컨택건수_리볼빙_인터넷_R6M', '컨택건수_카드론_당사앱_R6M', '컨택건수_CA_당사앱_R6M', '컨택건수_리볼빙_당사앱_R6M', '컨택건수_FDS_B0M', '컨택건수_FDS_R6M']\n",
            "------------------------------------<STEP_1 완료>------------------------------------\n",
            "------------------------------------<STEP_3 완료>------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from catboost import CatBoostClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "# 데이터 준비\n",
        "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]\n",
        "X = train_sampled_df[feature_cols].copy()\n",
        "y = train_sampled_df[\"Segment\"].copy()\n",
        "\n",
        "# 타깃 라벨 인코딩\n",
        "le_target = LabelEncoder()\n",
        "y_encoded = le_target.fit_transform(y)\n",
        "\n",
        "# 범주형 피처 정의 및 인코딩\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "X_test = evaluation_df.copy()\n",
        "y_true = X_test[\"Segment\"].copy()\n",
        "y_true_encoded = le_target.transform(y_true)\n",
        "X_test = X_test[feature_cols]\n",
        "\n",
        "encoders = {}\n",
        "for col in categorical_features:\n",
        "    le_train = LabelEncoder()\n",
        "    X[col] = le_train.fit_transform(X[col])\n",
        "    encoders[col] = le_train\n",
        "    X_test[col] = X_test[col].fillna('missing').astype(str)\n",
        "    unseen_labels_val = set(X_test[col]) - set(le_train.classes_)\n",
        "    if unseen_labels_val:\n",
        "        le_train.classes_ = np.append(le_train.classes_, list(unseen_labels_val))\n",
        "    X_test[col] = le_train.transform(X_test[col])\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# CatBoost 모델 학습 및 평가\n",
        "catboost_model = CatBoostClassifier(verbose=0, random_state=42)\n",
        "catboost_model.fit(X, y_encoded)\n",
        "y_pred_train = catboost_model.predict(X)\n",
        "f1_train = f1_score(y_encoded, y_pred_train, average='weighted')\n",
        "\n",
        "y_pred = catboost_model.predict(X_test)\n",
        "f1_test = f1_score(y_true_encoded, y_pred, average='weighted')\n",
        "\n",
        "print(f\"CatBoost - Train F1: {f1_train:.4f}, Test F1: {f1_test:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFQTOfIXgWaB",
        "outputId": "71054baf-8ae9-4b5d-bf6b-43655d49a1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-81b66571c801>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[col] = X_test[col].fillna('missing').astype(str)\n",
            "<ipython-input-4-81b66571c801>:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[col] = le_train.transform(X_test[col])\n",
            "<ipython-input-4-81b66571c801>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[col] = X_test[col].fillna('missing').astype(str)\n",
            "<ipython-input-4-81b66571c801>:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[col] = le_train.transform(X_test[col])\n",
            "<ipython-input-4-81b66571c801>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[col] = X_test[col].fillna('missing').astype(str)\n",
            "<ipython-input-4-81b66571c801>:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[col] = le_train.transform(X_test[col])\n",
            "<ipython-input-4-81b66571c801>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[col] = X_test[col].fillna('missing').astype(str)\n",
            "<ipython-input-4-81b66571c801>:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[col] = le_train.transform(X_test[col])\n",
            "<ipython-input-4-81b66571c801>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[col] = X_test[col].fillna('missing').astype(str)\n",
            "<ipython-input-4-81b66571c801>:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[col] = le_train.transform(X_test[col])\n",
            "<ipython-input-4-81b66571c801>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[col] = X_test[col].fillna('missing').astype(str)\n",
            "<ipython-input-4-81b66571c801>:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[col] = le_train.transform(X_test[col])\n",
            "<ipython-input-4-81b66571c801>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test[col] = X_test[col].fillna('missing').astype(str)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost - Train F1: 0.9637, Test F1: 0.8893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8x3cRUBwG4x"
      },
      "source": [
        "# TabNet(Jupyter에서 돌림)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W14cYc5Cfa87"
      },
      "outputs": [],
      "source": [
        "pip install rtdl -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8RhOipxmmxW"
      },
      "outputs": [],
      "source": [
        "# 1. 기존 PyTorch 제거\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip cache purge\n",
        "\n",
        "# 2. 안정적인 버전으로 재설치 (CUDA 11.8 지원)\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYhyl6CxwFG3"
      },
      "outputs": [],
      "source": [
        "# 필수 라이브러리\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "# 시드 고정 함수\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "# 데이터셋 준비\n",
        "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]\n",
        "X = train_sampled_df[feature_cols].copy()\n",
        "y = train_sampled_df[\"Segment\"].copy()\n",
        "\n",
        "le_target = LabelEncoder()\n",
        "y_encoded = le_target.fit_transform(y)\n",
        "\n",
        "X_test = evaluation_df[feature_cols].copy()\n",
        "y_true = evaluation_df[\"Segment\"].copy()\n",
        "y_true_encoded = le_target.transform(y_true)\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "encoders = {}\n",
        "for col in categorical_features:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str))\n",
        "    X_test[col] = X_test[col].fillna('missing').astype(str)\n",
        "    unseen = set(X_test[col]) - set(le.classes_)\n",
        "    if unseen:\n",
        "        le.classes_ = np.append(le.classes_, list(unseen))\n",
        "    X_test[col] = le.transform(X_test[col])\n",
        "    encoders[col] = le\n",
        "\n",
        "# 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# TabNet 학습 및 평가\n",
        "try:\n",
        "    print(\"Training TabNet...\")\n",
        "    clf_tabnet = TabNetClassifier(seed=42)\n",
        "    clf_tabnet.fit(\n",
        "        X.values, y_encoded,\n",
        "        eval_set=[(X_test.values, y_true_encoded)],\n",
        "        eval_name=['test'],\n",
        "        eval_metric=['accuracy'],\n",
        "        max_epochs=100,\n",
        "        patience=10,\n",
        "        batch_size=1024,\n",
        "        virtual_batch_size=128,\n",
        "        num_workers=0,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    y_pred_train_tabnet = clf_tabnet.predict(X.values)\n",
        "    y_pred_tabnet = clf_tabnet.predict(X_test.values)\n",
        "\n",
        "    f1_train_tabnet = f1_score(y_encoded, y_pred_train_tabnet, average='weighted')\n",
        "    f1_test_tabnet = f1_score(y_true_encoded, y_pred_tabnet, average='weighted')\n",
        "    print(f\"TabNet - Train F1: {f1_train_tabnet:.4f}, Test F1: {f1_test_tabnet:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"TabNet failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj5yF9j5wRzo"
      },
      "source": [
        "Training TabNet...\n",
        "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
        "  warnings.warn(f\"Device used : {self.device}\")\n",
        "epoch 0  | loss: 0.93222 | test_accuracy: 0.77705 |  0:00:23s\n",
        "epoch 1  | loss: 0.57087 | test_accuracy: 0.42175 |  0:00:50s\n",
        "epoch 2  | loss: 0.51032 | test_accuracy: 0.76045 |  0:01:04s\n",
        "epoch 3  | loss: 0.46622 | test_accuracy: 0.7983  |  0:01:18s\n",
        "epoch 4  | loss: 0.4302  | test_accuracy: 0.77005 |  0:01:32s\n",
        "epoch 5  | loss: 0.42396 | test_accuracy: 0.80905 |  0:01:51s\n",
        "epoch 6  | loss: 0.40947 | test_accuracy: 0.8293  |  0:02:05s\n",
        "epoch 7  | loss: 0.39964 | test_accuracy: 0.8361  |  0:02:19s\n",
        "epoch 8  | loss: 0.39249 | test_accuracy: 0.8378  |  0:02:32s\n",
        "epoch 9  | loss: 0.39234 | test_accuracy: 0.84085 |  0:02:46s\n",
        "epoch 10 | loss: 0.39019 | test_accuracy: 0.83985 |  0:03:01s\n",
        "epoch 11 | loss: 0.38356 | test_accuracy: 0.8408  |  0:03:15s\n",
        "epoch 12 | loss: 0.3766  | test_accuracy: 0.8423  |  0:03:30s\n",
        "epoch 13 | loss: 0.37444 | test_accuracy: 0.8405  |  0:03:43s\n",
        "epoch 14 | loss: 0.37173 | test_accuracy: 0.83625 |  0:03:59s\n",
        "epoch 15 | loss: 0.36827 | test_accuracy: 0.84145 |  0:04:14s\n",
        "epoch 16 | loss: 0.36452 | test_accuracy: 0.8391  |  0:04:31s\n",
        "epoch 17 | loss: 0.36625 | test_accuracy: 0.84135 |  0:04:46s\n",
        "epoch 18 | loss: 0.368   | test_accuracy: 0.84465 |  0:05:01s\n",
        "epoch 19 | loss: 0.36425 | test_accuracy: 0.84205 |  0:05:16s\n",
        "epoch 20 | loss: 0.3624  | test_accuracy: 0.84095 |  0:05:31s\n",
        "epoch 21 | loss: 0.35924 | test_accuracy: 0.84445 |  0:05:47s\n",
        "epoch 22 | loss: 0.35612 | test_accuracy: 0.84295 |  0:06:03s\n",
        "epoch 23 | loss: 0.35343 | test_accuracy: 0.8419  |  0:06:19s\n",
        "epoch 24 | loss: 0.3504  | test_accuracy: 0.83785 |  0:06:33s\n",
        "epoch 25 | loss: 0.34639 | test_accuracy: 0.8404  |  0:06:49s\n",
        "epoch 26 | loss: 0.34824 | test_accuracy: 0.82205 |  0:07:06s\n",
        "epoch 27 | loss: 0.34763 | test_accuracy: 0.84285 |  0:07:22s\n",
        "epoch 28 | loss: 0.34708 | test_accuracy: 0.8409  |  0:07:37s\n",
        "\n",
        "Early stopping occurred at epoch 28 with best_epoch = 18 and best_test_accuracy = 0.84465\n",
        "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
        "  warnings.warn(wrn_msg)\n",
        "TabNet - Train F1: 0.8345, Test F1: 0.8285\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgTVOKWRoGf-"
      },
      "source": [
        "#FT Transformer 미니배치X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKoIv_iOfKho",
        "outputId": "42bc4305-ff83-4a6c-92b6-f020bb6d98a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FT Transformer 학습 시작...\n"
          ]
        }
      ],
      "source": [
        "# 필수 라이브러리\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "import rtdl  # FT Transformer 라이브러리\n",
        "import torch.nn as nn\n",
        "\n",
        "# 시드 고정 함수\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "# 데이터셋 준비\n",
        "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]  # 피처 열 선택\n",
        "X = train_sampled_df[feature_cols].copy()  # 훈련 데이터 복사\n",
        "y = train_sampled_df[\"Segment\"].copy()  # 타겟 데이터 복사\n",
        "\n",
        "X_test = evaluation_df[feature_cols].copy()  # 테스트 데이터 복사\n",
        "y_true = evaluation_df[\"Segment\"].copy()  # 테스트 타겟 복사\n",
        "\n",
        "# 타겟 인코딩 (타겟은 여전히 숫자로 변환 필요)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le_target = LabelEncoder()  # 타겟 라벨 인코더\n",
        "y_encoded = le_target.fit_transform(y)  # 훈련 타겟 인코딩\n",
        "y_true_encoded = le_target.transform(y_true)  # 테스트 타겟 인코딩\n",
        "\n",
        "# 범주형 및 수치형 피처 분리\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()  # 범주형 피처 목록\n",
        "numerical_features = [col for col in feature_cols if col not in categorical_features]  # 수치형 피처 목록\n",
        "\n",
        "# 수치형 피처 스케일링\n",
        "scaler = StandardScaler()  # 표준화 스케일러\n",
        "X_num = scaler.fit_transform(X[numerical_features])  # 훈련 데이터 수치형 피처 스케일링\n",
        "X_test_num = scaler.transform(X_test[numerical_features])  # 테스트 데이터 수치형 피처 스케일링\n",
        "\n",
        "# 범주형 피처의 카디널리티 계산\n",
        "cat_cardinalities = [len(X[col].unique()) for col in categorical_features]  # 각 범주형 피처의 고유 값 개수\n",
        "\n",
        "# 범주형 피처를 정수형으로 변환 (0부터 시작하는 인덱스로)\n",
        "X_cat = np.stack([X[col].astype('category').cat.codes for col in categorical_features], axis=1)  # 훈련 데이터 범주형 변환\n",
        "X_test_cat = np.stack([X_test[col].fillna('missing').astype('category').cat.codes for col in categorical_features], axis=1)  # 테스트 데이터 범주형 변환\n",
        "\n",
        "# FT Transformer용 데이터 준비 (torch 텐서로 변환)\n",
        "X_num_tensor = torch.tensor(X_num, dtype=torch.float32)  # 훈련 수치형 데이터 텐서\n",
        "X_cat_tensor = torch.tensor(X_cat, dtype=torch.long) if categorical_features else None  # 훈련 범주형 데이터 텐서 (없으면 None)\n",
        "y_encoded_tensor = torch.tensor(y_encoded, dtype=torch.long)  # 훈련 타겟 텐서\n",
        "X_test_num_tensor = torch.tensor(X_test_num, dtype=torch.float32)  # 테스트 수치형 데이터 텐서\n",
        "X_test_cat_tensor = torch.tensor(X_test_cat, dtype=torch.long) if categorical_features else None  # 테스트 범주형 데이터 텐서\n",
        "y_true_encoded_tensor = torch.tensor(y_true_encoded, dtype=torch.long)  # 테스트 타겟 텐서\n",
        "\n",
        "\n",
        "# FT Transformer 학습 및 평가\n",
        "try:\n",
        "    print(\"FT Transformer 학습 시작...\")\n",
        "\n",
        "    # FT Transformer 모델 정의\n",
        "    d_out = 64  # FT Transformer 출력 차원 (임의로 64로 설정, 조정 가능)\n",
        "    ft_transformer = rtdl.FTTransformer.make_default(\n",
        "        n_num_features=len(numerical_features),  # 수치형 피처 수\n",
        "        cat_cardinalities=cat_cardinalities if categorical_features else None,  # 범주형 피처 카디널리티\n",
        "        d_out=d_out  # 출력 차원 명시\n",
        "    )\n",
        "\n",
        "    # 분류를 위한 출력 레이어 정의\n",
        "    n_classes = len(np.unique(y_encoded))  # 클래스 수\n",
        "    classifier = nn.Linear(d_out, n_classes)  # 출력 레이어\n",
        "\n",
        "    # 옵티마이저와 손실 함수 정의\n",
        "    optimizer = torch.optim.Adam(list(ft_transformer.parameters()) + list(classifier.parameters()), lr=1e-3)  # Adam 옵티마이저\n",
        "    loss_fn = nn.CrossEntropyLoss()  # 교차 엔트로피 손실 함수\n",
        "\n",
        "    # 장치 설정\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU 사용 여부 확인\n",
        "    ft_transformer.to(device)  # FT Transformer를 장치로 이동\n",
        "    classifier.to(device)  # 출력 레이어를 장치로 이동\n",
        "    X_num_tensor = X_num_tensor.to(device)  # 훈련 수치형 데이터 장치로 이동\n",
        "    X_cat_tensor = X_cat_tensor.to(device) if X_cat_tensor is not None else None  # 훈련 범주형 데이터 장치로 이동\n",
        "    y_encoded_tensor = y_encoded_tensor.to(device)  # 훈련 타겟 장치로 이동\n",
        "    X_test_num_tensor = X_test_num_tensor.to(device)  # 테스트 수치형 데이터 장치로 이동\n",
        "    X_test_cat_tensor = X_test_cat_tensor.to(device) if X_test_cat_tensor is not None else None  # 테스트 범주형 데이터 장치로 이동\n",
        "    y_true_encoded_tensor = y_true_encoded_tensor.to(device)  # 테스트 타겟 장치로 이동\n",
        "\n",
        "    # 학습 루프\n",
        "    max_epochs = 100  # 최대 에포크 수\n",
        "    patience = 10  # 조기 종료 인내심\n",
        "    best_f1 = -float('inf')  # 최고 F1 점수 초기화\n",
        "    patience_counter = 0  # 조기 종료 카운터\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        ft_transformer.train()  # 학습 모드\n",
        "        classifier.train()\n",
        "        optimizer.zero_grad()  # 기울기 초기화\n",
        "        transformer_output = ft_transformer(X_num_tensor, X_cat_tensor)  # FT Transformer 출력\n",
        "        outputs = classifier(transformer_output)  # 출력 레이어로 클래스 예측\n",
        "        loss = loss_fn(outputs, y_encoded_tensor)  # 손실 계산\n",
        "        loss.backward()  # 역전파\n",
        "        optimizer.step()  # 가중치 업데이트\n",
        "\n",
        "        # 평가\n",
        "        ft_transformer.eval()  # 평가 모드\n",
        "        classifier.eval()\n",
        "        with torch.no_grad():  # 기울기 계산 비활성화\n",
        "            test_transformer_output = ft_transformer(X_test_num_tensor, X_test_cat_tensor)  # 테스트 데이터로 FT Transformer 출력\n",
        "            y_pred_test = classifier(test_transformer_output).argmax(dim=1).cpu().numpy()  # 테스트 예측\n",
        "            f1_test = f1_score(y_true_encoded, y_pred_test, average='weighted')  # 테스트 F1 점수\n",
        "            print(f\"에포크 {epoch+1}/{max_epochs}, 테스트 F1: {f1_test:.4f}\")\n",
        "\n",
        "            # 조기 종료\n",
        "            if f1_test > best_f1:\n",
        "                best_f1 = f1_test  # 최고 F1 점수 갱신\n",
        "                patience_counter = 0  # 카운터 초기화\n",
        "            else:\n",
        "                patience_counter += 1  # 카운터 증가\n",
        "                if patience_counter >= patience:\n",
        "                    print(\"조기 종료가 실행되었습니다.\")\n",
        "                    break\n",
        "\n",
        "    # 최종 예측\n",
        "    ft_transformer.eval()  # 평가 모드\n",
        "    classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        train_transformer_output = ft_transformer(X_num_tensor, X_cat_tensor)\n",
        "        y_pred_train_ft = classifier(train_transformer_output).argmax(dim=1).cpu().numpy()  # 훈련 데이터 예측\n",
        "        test_transformer_output = ft_transformer(X_test_num_tensor, X_test_cat_tensor)\n",
        "        y_pred_test_ft = classifier(test_transformer_output).argmax(dim=1).cpu().numpy()  # 테스트 데이터 예측\n",
        "\n",
        "    f1_train_ft = f1_score(y_encoded, y_pred_train_ft, average='weighted')  # 훈련 F1 점수\n",
        "    f1_test_ft = f1_score(y_true_encoded, y_pred_test_ft, average='weighted')  # 테스트 F1 점수\n",
        "    print(f\"FT Transformer - 훈련 F1: {f1_train_ft:.4f}, 테스트 F1: {f1_test_ft:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"FT Transformer 실패: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3zIAUEOn-DW"
      },
      "source": [
        "#미니배치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko2srm3wjKxp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import rtdl\n",
        "\n",
        "# 시드 고정\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "# 데이터 전처리\n",
        "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]\n",
        "X = train_sampled_df[feature_cols].copy()\n",
        "y = train_sampled_df[\"Segment\"].copy()\n",
        "X_test = evaluation_df[feature_cols].copy()\n",
        "y_true = evaluation_df[\"Segment\"].copy()\n",
        "\n",
        "le_target = LabelEncoder()\n",
        "y_encoded = le_target.fit_transform(y)\n",
        "y_true_encoded = le_target.transform(y_true)\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_features = [col for col in feature_cols if col not in categorical_features]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_num = scaler.fit_transform(X[numerical_features])\n",
        "X_test_num = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "cat_cardinalities = [len(X[col].unique()) for col in categorical_features]\n",
        "\n",
        "X_cat = np.stack([X[col].astype('category').cat.codes for col in categorical_features], axis=1) if categorical_features else None\n",
        "X_test_cat = np.stack([X_test[col].fillna('missing').astype('category').cat.codes for col in categorical_features], axis=1) if categorical_features else None\n",
        "\n",
        "# torch 텐서 변환\n",
        "X_num_tensor = torch.tensor(X_num, dtype=torch.float32)\n",
        "X_cat_tensor = torch.tensor(X_cat, dtype=torch.long) if categorical_features else None\n",
        "y_encoded_tensor = torch.tensor(y_encoded, dtype=torch.long)\n",
        "\n",
        "X_test_num_tensor = torch.tensor(X_test_num, dtype=torch.float32)\n",
        "X_test_cat_tensor = torch.tensor(X_test_cat, dtype=torch.long) if categorical_features else None\n",
        "y_true_encoded_tensor = torch.tensor(y_true_encoded, dtype=torch.long)\n",
        "\n",
        "# Dataloader 준비\n",
        "train_dataset = TensorDataset(X_num_tensor, X_cat_tensor, y_encoded_tensor)\n",
        "test_dataset = TensorDataset(X_test_num_tensor, X_test_cat_tensor, y_true_encoded_tensor)\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 모델 정의\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "d_out = 64\n",
        "n_classes = len(np.unique(y_encoded))\n",
        "\n",
        "ft_transformer = rtdl.FTTransformer.make_default(\n",
        "    n_num_features=len(numerical_features),\n",
        "    cat_cardinalities=cat_cardinalities if categorical_features else None,\n",
        "    d_out=d_out,\n",
        ")\n",
        "classifier = nn.Linear(d_out, n_classes)\n",
        "\n",
        "ft_transformer.to(device)\n",
        "classifier.to(device)\n",
        "\n",
        "# 옵티마이저, 손실함수\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(ft_transformer.parameters()) + list(classifier.parameters()), lr=1e-3\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 학습 루프 (with 배치)\n",
        "max_epochs = 100\n",
        "patience = 10\n",
        "best_f1 = -float(\"inf\")\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    ft_transformer.train()\n",
        "    classifier.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_num_batch, X_cat_batch, y_batch in train_loader:\n",
        "        X_num_batch = X_num_batch.to(device)\n",
        "        X_cat_batch = X_cat_batch.to(device) if X_cat_batch is not None else None\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = ft_transformer(X_num_batch, X_cat_batch)\n",
        "        preds = classifier(output)\n",
        "        loss = loss_fn(preds, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # 평가\n",
        "    ft_transformer.eval()\n",
        "    classifier.eval()\n",
        "    y_pred_test = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_num_batch, X_cat_batch, _ in test_loader:\n",
        "            X_num_batch = X_num_batch.to(device)\n",
        "            X_cat_batch = X_cat_batch.to(device) if X_cat_batch is not None else None\n",
        "            logits = classifier(ft_transformer(X_num_batch, X_cat_batch))\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            y_pred_test.extend(preds.cpu().numpy())\n",
        "\n",
        "    f1_test = f1_score(y_true_encoded, y_pred_test, average='weighted')\n",
        "    print(f\"[{epoch+1:03d}] 테스트 F1: {f1_test:.4f}\")\n",
        "\n",
        "    # 조기 종료 체크\n",
        "    if f1_test > best_f1:\n",
        "        best_f1 = f1_test\n",
        "        patience_counter = 0\n",
        "        best_model = {\n",
        "            \"ft\": ft_transformer.state_dict(),\n",
        "            \"clf\": classifier.state_dict()\n",
        "        }\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"조기 종료 triggered.\")\n",
        "            break\n",
        "\n",
        "# 최종 평가\n",
        "print(f\"최종 테스트 F1 점수: {best_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBLEFGKhxc5_"
      },
      "source": [
        "# 미내배치+경량화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4roDx1noLKq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import rtdl\n",
        "\n",
        "# 시드 고정\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "# 데이터 전처리\n",
        "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]\n",
        "X = train_sampled_df[feature_cols].copy()\n",
        "y = train_sampled_df[\"Segment\"].copy()\n",
        "X_test = evaluation_df[feature_cols].copy()\n",
        "y_true = evaluation_df[\"Segment\"].copy()\n",
        "\n",
        "le_target = LabelEncoder()\n",
        "y_encoded = le_target.fit_transform(y)\n",
        "y_true_encoded = le_target.transform(y_true)\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_features = [col for col in feature_cols if col not in categorical_features]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_num = scaler.fit_transform(X[numerical_features])\n",
        "X_test_num = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "cat_cardinalities = [len(X[col].astype('category').cat.categories) for col in categorical_features]\n",
        "\n",
        "X_cat = np.stack([X[col].astype('category').cat.codes for col in categorical_features], axis=1) if categorical_features else None\n",
        "X_test_cat = np.stack([X_test[col].fillna('missing').astype('category').cat.codes for col in categorical_features], axis=1) if categorical_features else None\n",
        "\n",
        "# torch 텐서 변환\n",
        "X_num_tensor = torch.tensor(X_num, dtype=torch.float32)\n",
        "X_cat_tensor = torch.tensor(X_cat, dtype=torch.long) if categorical_features else None\n",
        "y_encoded_tensor = torch.tensor(y_encoded, dtype=torch.long)\n",
        "\n",
        "X_test_num_tensor = torch.tensor(X_test_num, dtype=torch.float32)\n",
        "X_test_cat_tensor = torch.tensor(X_test_cat, dtype=torch.long) if categorical_features else None\n",
        "y_true_encoded_tensor = torch.tensor(y_true_encoded, dtype=torch.long)\n",
        "\n",
        "# DataLoader 설정\n",
        "train_dataset = TensorDataset(X_num_tensor, X_cat_tensor, y_encoded_tensor)\n",
        "test_dataset = TensorDataset(X_test_num_tensor, X_test_cat_tensor, y_true_encoded_tensor)\n",
        "\n",
        "batch_size = 512\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=2)\n",
        "\n",
        "# 모델 정의\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "d_out = 32\n",
        "n_classes = len(np.unique(y_encoded))\n",
        "\n",
        "ft_transformer = rtdl.FTTransformer.make_default(\n",
        "    n_num_features=len(numerical_features),\n",
        "    cat_cardinalities=cat_cardinalities if categorical_features else None,\n",
        "    d_out=d_out,\n",
        ")\n",
        "classifier = nn.Linear(d_out, n_classes)\n",
        "\n",
        "ft_transformer.to(device)\n",
        "classifier.to(device)\n",
        "\n",
        "# Optimizer & Loss\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(ft_transformer.parameters()) + list(classifier.parameters()), lr=1e-3\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# 학습 루프\n",
        "max_epochs = 50\n",
        "patience = 5\n",
        "best_f1 = -float(\"inf\")\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    ft_transformer.train()\n",
        "    classifier.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_num_batch, X_cat_batch, y_batch in train_loader:\n",
        "        X_num_batch = X_num_batch.to(device)\n",
        "        X_cat_batch = X_cat_batch.to(device) if X_cat_batch is not None else None\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = ft_transformer(X_num_batch, X_cat_batch)\n",
        "        preds = classifier(output)\n",
        "        loss = loss_fn(preds, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # 평가\n",
        "    ft_transformer.eval()\n",
        "    classifier.eval()\n",
        "    y_pred_test = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_num_batch, X_cat_batch, _ in test_loader:\n",
        "            X_num_batch = X_num_batch.to(device)\n",
        "            X_cat_batch = X_cat_batch.to(device) if X_cat_batch is not None else None\n",
        "            logits = classifier(ft_transformer(X_num_batch, X_cat_batch))\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            y_pred_test.extend(preds.cpu().numpy())\n",
        "\n",
        "    f1_test = f1_score(y_true_encoded, y_pred_test, average='weighted')\n",
        "    print(f\"[{epoch+1:03d}] 테스트 F1: {f1_test:.4f}\")\n",
        "\n",
        "    # 조기 종료 조건\n",
        "    if f1_test > best_f1:\n",
        "        best_f1 = f1_test\n",
        "        patience_counter = 0\n",
        "        best_model = {\n",
        "            \"ft\": ft_transformer.state_dict(),\n",
        "            \"clf\": classifier.state_dict()\n",
        "        }\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"조기 종료 triggered.\")\n",
        "            break\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# -----------------------------\n",
        "# 최종 결과 출력\n",
        "# -----------------------------\n",
        "print(f\"최종 테스트 F1 점수: {best_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNQNZTBHeXqx"
      },
      "source": [
        "LIGHTGBM 추가해서 해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6zy-YWru2Uy"
      },
      "source": [
        "# XGBoost Random Forest AdaBoost Decision Tree KNN SVM Logistic Regression CatBoost DNN MLP CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxSb5A7VxH-y"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier  # CatBoost 추가\n",
        "from hpelm import ELM\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "# 데이터 준비\n",
        "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]\n",
        "X = train_sampled_df[feature_cols].copy()\n",
        "y = train_sampled_df[\"Segment\"].copy()\n",
        "\n",
        "# 타깃 라벨 인코딩\n",
        "le_target = LabelEncoder()\n",
        "y_encoded = le_target.fit_transform(y)\n",
        "\n",
        "# 범주형 피처 정의 및 인코딩\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# 테스트 데이터 준비\n",
        "X_test = evaluation_df.copy()\n",
        "y_true = X_test[\"Segment\"].copy()\n",
        "y_true_encoded = le_target.transform(y_true)\n",
        "X_test = X_test[feature_cols]\n",
        "\n",
        "encoders = {}\n",
        "for col in categorical_features:\n",
        "    le_train = LabelEncoder()\n",
        "    X[col] = le_train.fit_transform(X[col])\n",
        "    encoders[col] = le_train\n",
        "    X_test[col] = X_test[col].fillna('missing').astype(str)\n",
        "    unseen_labels_val = set(X_test[col]) - set(le_train.classes_)\n",
        "    if unseen_labels_val:\n",
        "        le_train.classes_ = np.append(le_train.classes_, list(unseen_labels_val))\n",
        "    X_test[col] = le_train.transform(X_test[col])\n",
        "\n",
        "# 데이터 스케일링 (KNN, SVM, Logistic Regression, MLP, CNN 등에 필요)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 메모리 정리\n",
        "gc.collect()\n",
        "\n",
        "# 모델 리스트 정의\n",
        "models = {\n",
        "    \"XGBoost\": xgb.XGBClassifier(tree_method='hist', random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"SVM\": SVC(random_state=42),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42),\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=42)  # CatBoost 추가\n",
        "}\n",
        "\n",
        "# DNN 모델 정의\n",
        "def create_dnn(input_dim, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# ANN 모델 정의\n",
        "def create_ann(input_dim, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# MLP 모델 정의 (더 깊은 ANN)\n",
        "def create_mlp(input_dim, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# CNN 모델 정의 (1D CNN for tabular data)\n",
        "def create_cnn(input_dim, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(input_dim, 1)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 모델 학습 및 평가 (과적합 판단 포함)\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        print(f\"Training {name}...\")\n",
        "        model.fit(X_scaled if name in [\"KNN\", \"SVM\", \"Logistic Regression\"] else X, y_encoded)\n",
        "        # 훈련 데이터 성능\n",
        "        y_pred_train = model.predict(X_scaled if name in [\"KNN\", \"SVM\", \"Logistic Regression\"] else X)\n",
        "        f1_train = f1_score(y_encoded, y_pred_train, average='weighted')\n",
        "        # 테스트 데이터 성능\n",
        "        y_pred = model.predict(X_test_scaled if name in [\"KNN\", \"SVM\", \"Logistic Regression\"] else X_test)\n",
        "        f1_test = f1_score(y_true_encoded, y_pred, average='weighted')\n",
        "        results[name] = {\"Train F1\": f1_train, \"Test F1\": f1_test}\n",
        "        print(f\"{name} - Train F1: {f1_train:.4f}, Test F1: {f1_test:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{name} failed: {e}\")\n",
        "\n",
        "# ELM 처리\n",
        "try:\n",
        "    print(\"Training ELM...\")\n",
        "    elm = ELM(X.shape[1], len(np.unique(y_encoded)), classification=\"multiclass\", batch=1000)\n",
        "    elm.add_neurons(100, \"sigm\")\n",
        "    elm.train(X, y_encoded.reshape(-1, 1))\n",
        "    y_pred_elm = elm.predict(X_test).argmax(axis=1)\n",
        "    f1_train_elm = f1_score(y_encoded, elm.predict(X).argmax(axis=1), average='weighted')\n",
        "    f1_test_elm = f1_score(y_true_encoded, y_pred_elm, average='weighted')\n",
        "    results[\"ELM\"] = {\"Train F1\": f1_train_elm, \"Test F1\": f1_test_elm}\n",
        "    print(f\"ELM - Train F1: {f1_train_elm:.4f}, Test F1: {f1_test_elm:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"ELM failed: {e}\")\n",
        "\n",
        "# DNN 학습 및 평가\n",
        "try:\n",
        "    print(\"Training DNN...\")\n",
        "    dnn_model = create_dnn(X.shape[1], len(np.unique(y_encoded)))\n",
        "    dnn_model.fit(X_scaled, y_encoded, epochs=20, batch_size=32, verbose=0)\n",
        "    y_pred_train_dnn = np.argmax(dnn_model.predict(X_scaled), axis=1)\n",
        "    y_pred_dnn = np.argmax(dnn_model.predict(X_test_scaled), axis=1)\n",
        "    f1_train_dnn = f1_score(y_encoded, y_pred_train_dnn, average='weighted')\n",
        "    f1_test_dnn = f1_score(y_true_encoded, y_pred_dnn, average='weighted')\n",
        "    results[\"DNN\"] = {\"Train F1\": f1_train_dnn, \"Test F1\": f1_test_dnn}\n",
        "    print(f\"DNN - Train F1: {f1_train_dnn:.4f}, Test F1: {f1_test_dnn:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"DNN failed: {e}\")\n",
        "\n",
        "# ANN 학습 및 평가\n",
        "try:\n",
        "    print(\"Training ANN...\")\n",
        "    ann_model = create_ann(X.shape[1], len(np.unique(y_encoded)))\n",
        "    ann_model.fit(X_scaled, y_encoded, epochs=20, batch_size=32, verbose=0)\n",
        "    y_pred_train_ann = np.argmax(ann_model.predict(X_scaled), axis=1)\n",
        "    y_pred_ann = np.argmax(ann_model.predict(X_test_scaled), axis=1)\n",
        "    f1_train_ann = f1_score(y_encoded, y_pred_train_ann, average='weighted')\n",
        "    f1_test_ann = f1_score(y_true_encoded, y_pred_ann, average='weighted')\n",
        "    results[\"ANN\"] = {\"Train F1\": f1_train_ann, \"Test F1\": f1_test_ann}\n",
        "    print(f\"ANN - Train F1: {f1_train_ann:.4f}, Test F1: {f1_test_ann:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"ANN failed: {e}\")\n",
        "\n",
        "# MLP 학습 및 평가\n",
        "try:\n",
        "    print(\"Training MLP...\")\n",
        "    mlp_model = create_mlp(X.shape[1], len(np.unique(y_encoded)))\n",
        "    mlp_model.fit(X_scaled, y_encoded, epochs=20, batch_size=32, verbose=0)\n",
        "    y_pred_train_mlp = np.argmax(mlp_model.predict(X_scaled), axis=1)\n",
        "    y_pred_mlp = np.argmax(mlp_model.predict(X_test_scaled), axis=1)\n",
        "    f1_train_mlp = f1_score(y_encoded, y_pred_train_mlp, average='weighted')\n",
        "    f1_test_mlp = f1_score(y_true_encoded, y_pred_mlp, average='weighted')\n",
        "    results[\"MLP\"] = {\"Train F1\": f1_train_mlp, \"Test F1\": f1_test_mlp}\n",
        "    print(f\"MLP - Train F1: {f1_train_mlp:.4f}, Test F1: {f1_test_mlp:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"MLP failed: {e}\")\n",
        "\n",
        "# CNN 학습 및 평가 (1D CNN을 위해 데이터 reshape 필요)\n",
        "try:\n",
        "    print(\"Training CNN...\")\n",
        "    X_cnn = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
        "    X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "    cnn_model = create_cnn(X_scaled.shape[1], len(np.unique(y_encoded)))\n",
        "    cnn_model.fit(X_cnn, y_encoded, epochs=20, batch_size=32, verbose=0)\n",
        "    y_pred_train_cnn = np.argmax(cnn_model.predict(X_cnn), axis=1)\n",
        "    y_pred_cnn = np.argmax(cnn_model.predict(X_test_cnn), axis=1)\n",
        "    f1_train_cnn = f1_score(y_encoded, y_pred_train_cnn, average='weighted')\n",
        "    f1_test_cnn = f1_score(y_true_encoded, y_pred_cnn, average='weighted')\n",
        "    results[\"CNN\"] = {\"Train F1\": f1_train_cnn, \"Test F1\": f1_test_cnn}\n",
        "    print(f\"CNN - Train F1: {f1_train_cnn:.4f}, Test F1: {f1_test_cnn:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"CNN failed: {e}\")\n",
        "\n",
        "\n",
        "# 결과 정리 및 최고 모델 선정\n",
        "print(\"\\n=== Model Performance ===\")\n",
        "for name, scores in results.items():\n",
        "    print(f\"{name}: Train F1 = {scores['Train F1']:.4f}, Test F1 = {scores['Test F1']:.4f}, \"\n",
        "          f\"Difference = {abs(scores['Train F1'] - scores['Test F1']):.4f}\")\n",
        "\n",
        "best_model = max(results, key=lambda x: results[x][\"Test F1\"])\n",
        "print(f\"\\nBest Model: {best_model} with Test F1-Score: {results[best_model]['Test F1']:.4f}\")\n",
        "\n",
        "# 결과 DataFrame으로 저장\n",
        "results_df = pd.DataFrame(\n",
        "    [(name, scores[\"Train F1\"], scores[\"Test F1\"]) for name, scores in results.items()],\n",
        "    columns=[\"Model\", \"Train F1-Score\", \"Test F1-Score\"]\n",
        ")\n",
        "print(\"\\nResults Table:\\n\", results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE6GyCAdBrUD",
        "outputId": "29892aa6-0177-42e7-c537-53e3dc14551a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original features: 851, PCA features: 262\n",
            "Training XGBoost...\n",
            "XGBoost - Train F1: 1.0000, Test F1: 0.8367\n",
            "Training Random Forest...\n",
            "Random Forest - Train F1: 1.0000, Test F1: 0.8050\n",
            "Training AdaBoost...\n",
            "AdaBoost - Train F1: 0.7999, Test F1: 0.7947\n",
            "Training Decision Tree...\n",
            "Decision Tree - Train F1: 1.0000, Test F1: 0.7737\n",
            "Training KNN...\n",
            "KNN - Train F1: 0.8603, Test F1: 0.8088\n",
            "Training SVM...\n",
            "SVM - Train F1: 0.9266, Test F1: 0.8443\n",
            "Training Logistic Regression...\n",
            "Logistic Regression - Train F1: 0.8700, Test F1: 0.8539\n",
            "Training CatBoost...\n",
            "CatBoost - Train F1: 0.9772, Test F1: 0.8444\n",
            "Training ELM...\n",
            "ELM - Train F1: 0.7848, Test F1: 0.7829\n",
            "Training DNN...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 870us/step\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 870us/step\n",
            "DNN - Train F1: 0.9646, Test F1: 0.8525\n",
            "Training ANN...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 860us/step\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step\n",
            "ANN failed: name 'fを表示_train_ann' is not defined\n",
            "Training MLP...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "MLP - Train F1: 0.9734, Test F1: 0.8468\n",
            "Training CNN...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
            "CNN - Train F1: 0.9855, Test F1: 0.8314\n",
            "\n",
            "=== Model Performance ===\n",
            "XGBoost: Train F1 = 1.0000, Test F1 = 0.8367, Difference = 0.1633\n",
            "Random Forest: Train F1 = 1.0000, Test F1 = 0.8050, Difference = 0.1950\n",
            "AdaBoost: Train F1 = 0.7999, Test F1 = 0.7947, Difference = 0.0052\n",
            "Decision Tree: Train F1 = 1.0000, Test F1 = 0.7737, Difference = 0.2263\n",
            "KNN: Train F1 = 0.8603, Test F1 = 0.8088, Difference = 0.0514\n",
            "SVM: Train F1 = 0.9266, Test F1 = 0.8443, Difference = 0.0824\n",
            "Logistic Regression: Train F1 = 0.8700, Test F1 = 0.8539, Difference = 0.0161\n",
            "CatBoost: Train F1 = 0.9772, Test F1 = 0.8444, Difference = 0.1328\n",
            "ELM: Train F1 = 0.7848, Test F1 = 0.7829, Difference = 0.0019\n",
            "DNN: Train F1 = 0.9646, Test F1 = 0.8525, Difference = 0.1121\n",
            "MLP: Train F1 = 0.9734, Test F1 = 0.8468, Difference = 0.1266\n",
            "CNN: Train F1 = 0.9855, Test F1 = 0.8314, Difference = 0.1541\n",
            "\n",
            "Best Model: Logistic Regression with Test F1-Score: 0.8539\n",
            "\n",
            "Results Table:\n",
            "                   Model  Train F1-Score  Test F1-Score\n",
            "0               XGBoost        1.000000       0.836676\n",
            "1         Random Forest        1.000000       0.804966\n",
            "2              AdaBoost        0.799916       0.794748\n",
            "3         Decision Tree        1.000000       0.773689\n",
            "4                   KNN        0.860264       0.808820\n",
            "5                   SVM        0.926644       0.844266\n",
            "6   Logistic Regression        0.869970       0.853865\n",
            "7              CatBoost        0.977204       0.844355\n",
            "8                   ELM        0.784794       0.782905\n",
            "9                   DNN        0.964592       0.852524\n",
            "10                  MLP        0.973438       0.846821\n",
            "11                  CNN        0.985456       0.831350\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "from hpelm import ELM  # ELM 추가\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "# 데이터 준비\n",
        "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]\n",
        "X = train_sampled_df[feature_cols].copy()\n",
        "y = train_sampled_df[\"Segment\"].copy()\n",
        "\n",
        "# 타깃 라벨 인코딩\n",
        "le_target = LabelEncoder()\n",
        "y_encoded = le_target.fit_transform(y)\n",
        "\n",
        "# 범주형 피처 인코딩\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "encoders = {}\n",
        "for col in categorical_features:\n",
        "    le_train = LabelEncoder()\n",
        "    X[col] = le_train.fit_transform(X[col])\n",
        "    encoders[col] = le_train\n",
        "\n",
        "# 테스트 데이터 준비\n",
        "X_test = evaluation_df.copy()\n",
        "y_true = X_test[\"Segment\"].copy()\n",
        "y_true_encoded = le_target.transform(y_true)\n",
        "X_test = X_test[feature_cols]\n",
        "for col in categorical_features:\n",
        "    X_test[col] = X_test[col].fillna('missing').astype(str)\n",
        "    unseen_labels_val = set(X_test[col]) - set(encoders[col].classes_)\n",
        "    if unseen_labels_val:\n",
        "        encoders[col].classes_ = np.append(encoders[col].classes_, list(unseen_labels_val))\n",
        "    X_test[col] = encoders[col].transform(X_test[col])\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# PCA 적용 (95% 분산 설명)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "print(f\"Original features: {X.shape[1]}, PCA features: {X_pca.shape[1]}\")\n",
        "\n",
        "# 메모리 정리\n",
        "gc.collect()\n",
        "\n",
        "# 모델 리스트 정의\n",
        "models = {\n",
        "    \"XGBoost\": xgb.XGBClassifier(tree_method='hist', random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"SVM\": SVC(random_state=42),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42),\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=42)\n",
        "}\n",
        "\n",
        "# DNN 모델 정의\n",
        "def create_dnn(input_dim, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# ANN 모델 정의\n",
        "def create_ann(input_dim, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# MLP 모델 정의\n",
        "def create_mlp(input_dim, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# CNN 모델 정의\n",
        "def create_cnn(input_dim, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(input_dim, 1)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 모델 학습 및 평가 (과적합 판단 포함)\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        print(f\"Training {name}...\")\n",
        "        model.fit(X_pca, y_encoded)\n",
        "        y_pred_train = model.predict(X_pca)\n",
        "        y_pred = model.predict(X_test_pca)\n",
        "        f1_train = f1_score(y_encoded, y_pred_train, average='weighted')\n",
        "        f1_test = f1_score(y_true_encoded, y_pred, average='weighted')\n",
        "        results[name] = {\"Train F1\": f1_train, \"Test F1\": f1_test}\n",
        "        print(f\"{name} - Train F1: {f1_train:.4f}, Test F1: {f1_test:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{name} failed: {e}\")\n",
        "\n",
        "# ELM 학습 및 평가\n",
        "try:\n",
        "    print(\"Training ELM...\")\n",
        "    elm = ELM(X_pca.shape[1], len(np.unique(y_encoded)), classification=\"multiclass\", batch=1000)\n",
        "    elm.add_neurons(100, \"sigm\")  # 100개 뉴런, 시그모이드 활성화\n",
        "    # y_encoded를 2D 배열로 변환 (ELM 요구사항)\n",
        "    y_elm = np.eye(len(np.unique(y_encoded)))[y_encoded]  # 원-핫 인코딩\n",
        "    elm.train(X_pca, y_elm)\n",
        "    y_pred_train_elm = elm.predict(X_pca).argmax(axis=1)\n",
        "    y_pred_elm = elm.predict(X_test_pca).argmax(axis=1)\n",
        "    f1_train_elm = f1_score(y_encoded, y_pred_train_elm, average='weighted')\n",
        "    f1_test_elm = f1_score(y_true_encoded, y_pred_elm, average='weighted')\n",
        "    results[\"ELM\"] = {\"Train F1\": f1_train_elm, \"Test F1\": f1_test_elm}\n",
        "    print(f\"ELM - Train F1: {f1_train_elm:.4f}, Test F1: {f1_test_elm:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"ELM failed: {e}\")\n",
        "\n",
        "# DNN 학습 및 평가\n",
        "try:\n",
        "    print(\"Training DNN...\")\n",
        "    dnn_model = create_dnn(X_pca.shape[1], len(np.unique(y_encoded)))\n",
        "    dnn_model.fit(X_pca, y_encoded, epochs=20, batch_size=32, verbose=0)\n",
        "    y_pred_train_dnn = np.argmax(dnn_model.predict(X_pca), axis=1)\n",
        "    y_pred_dnn = np.argmax(dnn_model.predict(X_test_pca), axis=1)\n",
        "    f1_train_dnn = f1_score(y_encoded, y_pred_train_dnn, average='weighted')\n",
        "    f1_test_dnn = f1_score(y_true_encoded, y_pred_dnn, average='weighted')\n",
        "    results[\"DNN\"] = {\"Train F1\": f1_train_dnn, \"Test F1\": f1_test_dnn}\n",
        "    print(f\"DNN - Train F1: {f1_train_dnn:.4f}, Test F1: {f1_test_dnn:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"DNN failed: {e}\")\n",
        "\n",
        "# ANN 학습 및 평가\n",
        "try:\n",
        "    print(\"Training ANN...\")\n",
        "    ann_model = create_ann(X_pca.shape[1], len(np.unique(y_encoded)))\n",
        "    ann_model.fit(X_pca, y_encoded, epochs=20, batch_size=32, verbose=0)\n",
        "    y_pred_train_ann = np.argmax(ann_model.predict(X_pca), axis=1)\n",
        "    y_pred_ann = np.argmax(ann_model.predict(X_test_pca), axis=1)\n",
        "    f1_train_ann = f1_score(y_encoded, y_pred_train_ann, average='weighted')\n",
        "    f1_test_ann = f1_score(y_true_encoded, y_pred_ann, average='weighted')\n",
        "    results[\"ANN\"] = {\"Train F1\": fを表示_train_ann, \"Test F1\": f1_test_ann}\n",
        "    print(f\"ANN - Train F1: {f1_train_ann:.4f}, Test F1: {f1_test_ann:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"ANN failed: {e}\")\n",
        "\n",
        "# MLP 학습 및 평가\n",
        "try:\n",
        "    print(\"Training MLP...\")\n",
        "    mlp_model = create_mlp(X_pca.shape[1], len(np.unique(y_encoded)))\n",
        "    mlp_model.fit(X_pca, y_encoded, epochs=20, batch_size=32, verbose=0)\n",
        "    y_pred_train_mlp = np.argmax(mlp_model.predict(X_pca), axis=1)\n",
        "    y_pred_mlp = np.argmax(mlp_model.predict(X_test_pca), axis=1)\n",
        "    f1_train_mlp = f1_score(y_encoded, y_pred_train_mlp, average='weighted')\n",
        "    f1_test_mlp = f1_score(y_true_encoded, y_pred_mlp, average='weighted')\n",
        "    results[\"MLP\"] = {\"Train F1\": f1_train_mlp, \"Test F1\": f1_test_mlp}\n",
        "    print(f\"MLP - Train F1: {f1_train_mlp:.4f}, Test F1: {f1_test_mlp:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"MLP failed: {e}\")\n",
        "\n",
        "# CNN 학습 및 평가\n",
        "try:\n",
        "    print(\"Training CNN...\")\n",
        "    X_pca_cnn = X_pca.reshape((X_pca.shape[0], X_pca.shape[1], 1))\n",
        "    X_test_pca_cnn = X_test_pca.reshape((X_test_pca.shape[0], X_test_pca.shape[1], 1))\n",
        "    cnn_model = create_cnn(X_pca.shape[1], len(np.unique(y_encoded)))\n",
        "    cnn_model.fit(X_pca_cnn, y_encoded, epochs=20, batch_size=32, verbose=0)\n",
        "    y_pred_train_cnn = np.argmax(cnn_model.predict(X_pca_cnn), axis=1)\n",
        "    y_pred_cnn = np.argmax(cnn_model.predict(X_test_pca_cnn), axis=1)\n",
        "    f1_train_cnn = f1_score(y_encoded, y_pred_train_cnn, average='weighted')\n",
        "    f1_test_cnn = f1_score(y_true_encoded, y_pred_cnn, average='weighted')\n",
        "    results[\"CNN\"] = {\"Train F1\": f1_train_cnn, \"Test F1\": f1_test_cnn}\n",
        "    print(f\"CNN - Train F1: {f1_train_cnn:.4f}, Test F1: {f1_test_cnn:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"CNN failed: {e}\")\n",
        "\n",
        "# 결과 정리\n",
        "print(\"\\n=== Model Performance ===\")\n",
        "for name, scores in results.items():\n",
        "    print(f\"{name}: Train F1 = {scores['Train F1']:.4f}, Test F1 = {scores['Test F1']:.4f}, \"\n",
        "          f\"Difference = {abs(scores['Train F1'] - scores['Test F1']):.4f}\")\n",
        "\n",
        "best_model = max(results, key=lambda x: results[x][\"Test F1\"])\n",
        "print(f\"\\nBest Model: {best_model} with Test F1-Score: {results[best_model]['Test F1']:.4f}\")\n",
        "\n",
        "# 결과 DataFrame\n",
        "results_df = pd.DataFrame(\n",
        "    [(name, scores[\"Train F1\"], scores[\"Test F1\"]) for name, scores in results.items()],\n",
        "    columns=[\"Model\", \"Train F1-Score\", \"Test F1-Score\"]\n",
        ")\n",
        "print(\"\\nResults Table:\\n\", results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH_eMer2GJS8",
        "outputId": "f9d1f1c5-62f6-4e46-8ebe-e22f2fee6e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original features: 851, PCA features: 419\n",
            "Training XGBoost...\n",
            "XGBoost - Train F1: 1.0000, Test F1: 0.8393\n",
            "Training Random Forest...\n",
            "Random Forest - Train F1: 1.0000, Test F1: 0.7919\n",
            "Training AdaBoost...\n",
            "AdaBoost - Train F1: 0.8010, Test F1: 0.7931\n",
            "Training Decision Tree...\n",
            "Decision Tree - Train F1: 1.0000, Test F1: 0.7714\n",
            "Training KNN...\n",
            "KNN - Train F1: 0.8590, Test F1: 0.8082\n",
            "Training SVM...\n",
            "SVM - Train F1: 0.9338, Test F1: 0.8467\n",
            "Training Logistic Regression...\n",
            "Logistic Regression - Train F1: 0.8889, Test F1: 0.8645\n",
            "Training CatBoost...\n",
            "CatBoost - Train F1: 0.9819, Test F1: 0.8478\n",
            "Training ELM...\n",
            "ELM - Train F1: 0.7905, Test F1: 0.7866\n",
            "Training DNN...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "DNN - Train F1: 0.9857, Test F1: 0.8623\n",
            "Training ANN...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 866us/step\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 839us/step\n",
            "ANN failed: name 'fを表示_train_ann' is not defined\n",
            "Training MLP...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "MLP - Train F1: 0.9935, Test F1: 0.8592\n",
            "Training CNN...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step\n",
            "CNN - Train F1: 0.9956, Test F1: 0.8376\n",
            "\n",
            "=== Model Performance ===\n",
            "XGBoost: Train F1 = 1.0000, Test F1 = 0.8393, Difference = 0.1607\n",
            "Random Forest: Train F1 = 1.0000, Test F1 = 0.7919, Difference = 0.2081\n",
            "AdaBoost: Train F1 = 0.8010, Test F1 = 0.7931, Difference = 0.0079\n",
            "Decision Tree: Train F1 = 1.0000, Test F1 = 0.7714, Difference = 0.2286\n",
            "KNN: Train F1 = 0.8590, Test F1 = 0.8082, Difference = 0.0508\n",
            "SVM: Train F1 = 0.9338, Test F1 = 0.8467, Difference = 0.0870\n",
            "Logistic Regression: Train F1 = 0.8889, Test F1 = 0.8645, Difference = 0.0245\n",
            "CatBoost: Train F1 = 0.9819, Test F1 = 0.8478, Difference = 0.1341\n",
            "ELM: Train F1 = 0.7905, Test F1 = 0.7866, Difference = 0.0040\n",
            "DNN: Train F1 = 0.9857, Test F1 = 0.8623, Difference = 0.1234\n",
            "MLP: Train F1 = 0.9935, Test F1 = 0.8592, Difference = 0.1343\n",
            "CNN: Train F1 = 0.9956, Test F1 = 0.8376, Difference = 0.1579\n",
            "\n",
            "Best Model: Logistic Regression with Test F1-Score: 0.8645\n",
            "\n",
            "Results Table:\n",
            "                   Model  Train F1-Score  Test F1-Score\n",
            "0               XGBoost        1.000000       0.839258\n",
            "1         Random Forest        1.000000       0.791911\n",
            "2              AdaBoost        0.801019       0.793142\n",
            "3         Decision Tree        1.000000       0.771442\n",
            "4                   KNN        0.859004       0.808250\n",
            "5                   SVM        0.933782       0.846740\n",
            "6   Logistic Regression        0.888948       0.864497\n",
            "7              CatBoost        0.981872       0.847766\n",
            "8                   ELM        0.790544       0.786565\n",
            "9                   DNN        0.985742       0.862292\n",
            "10                  MLP        0.993450       0.859195\n",
            "11                  CNN        0.995573       0.837635\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "from hpelm import ELM  # ELM 추가\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "# 데이터 준비\n",
        "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]\n",
        "X = train_sampled_df[feature_cols].copy()\n",
        "y = train_sampled_df[\"Segment\"].copy()\n",
        "\n",
        "# 타깃 라벨 인코딩\n",
        "le_target = LabelEncoder()\n",
        "y_encoded = le_target.fit_transform(y)\n",
        "\n",
        "# 범주형 피처 인코딩\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "encoders = {}\n",
        "for col in categorical_features:\n",
        "    le_train = LabelEncoder()\n",
        "    X[col] = le_train.fit_transform(X[col])\n",
        "    encoders[col] = le_train\n",
        "\n",
        "# 테스트 데이터 준비\n",
        "X_test = evaluation_df.copy()\n",
        "y_true = X_test[\"Segment\"].copy()\n",
        "y_true_encoded = le_target.transform(y_true)\n",
        "X_test = X_test[feature_cols]\n",
        "for col in categorical_features:\n",
        "    X_test[col] = X_test[col].fillna('missing').astype(str)\n",
        "    unseen_labels_val = set(X_test[col]) - set(encoders[col].classes_)\n",
        "    if unseen_labels_val:\n",
        "        encoders[col].classes_ = np.append(encoders[col].classes_, list(unseen_labels_val))\n",
        "    X_test[col] = encoders[col].transform(X_test[col])\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# PCA 적용\n",
        "pca = PCA(n_components=0.99)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "print(f\"Original features: {X.shape[1]}, PCA features: {X_pca.shape[1]}\")\n",
        "\n",
        "# 메모리 정리\n",
        "gc.collect()\n",
        "\n",
        "# 모델 리스트 정의\n",
        "models = {\n",
        "    \"XGBoost\": xgb.XGBClassifier(tree_method='hist', random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"SVM\": SVC(random_state=42),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42),\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=42)\n",
        "}\n",
        "\n",
        "# DNN 모델 정의\n",
        "def create_dnn(input_dim, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# ANN 모델 정의\n",
        "def create_ann(input_dim, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# MLP 모델 정의\n",
        "def create_mlp(input_dim, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# CNN 모델 정의\n",
        "def create_cnn(input_dim, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(input_dim, 1)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 모델 학습 및 평가 (과적합 판단 포함)\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        print(f\"Training {name}...\")\n",
        "        model.fit(X_pca, y_encoded)\n",
        "        y_pred_train = model.predict(X_pca)\n",
        "        y_pred = model.predict(X_test_pca)\n",
        "        f1_train = f1_score(y_encoded, y_pred_train, average='weighted')\n",
        "        f1_test = f1_score(y_true_encoded, y_pred, average='weighted')\n",
        "        results[name] = {\"Train F1\": f1_train, \"Test F1\": f1_test}\n",
        "        print(f\"{name} - Train F1: {f1_train:.4f}, Test F1: {f1_test:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{name} failed: {e}\")\n",
        "\n",
        "# ELM 학습 및 평가\n",
        "try:\n",
        "    print(\"Training ELM...\")\n",
        "    elm = ELM(X_pca.shape[1], len(np.unique(y_encoded)), classification=\"multiclass\", batch=1000)\n",
        "    elm.add_neurons(100, \"sigm\")  # 100개 뉴런, 시그모이드 활성화\n",
        "    # y_encoded를 2D 배열로 변환 (ELM 요구사항)\n",
        "    y_elm = np.eye(len(np.unique(y_encoded)))[y_encoded]  # 원-핫 인코딩\n",
        "    elm.train(X_pca, y_elm)\n",
        "    y_pred_train_elm = elm.predict(X_pca).argmax(axis=1)\n",
        "    y_pred_elm = elm.predict(X_test_pca).argmax(axis=1)\n",
        "    f1_train_elm = f1_score(y_encoded, y_pred_train_elm, average='weighted')\n",
        "    f1_test_elm = f1_score(y_true_encoded, y_pred_elm, average='weighted')\n",
        "    results[\"ELM\"] = {\"Train F1\": f1_train_elm, \"Test F1\": f1_test_elm}\n",
        "    print(f\"ELM - Train F1: {f1_train_elm:.4f}, Test F1: {f1_test_elm:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"ELM failed: {e}\")\n",
        "\n",
        "# DNN 학습 및 평가\n",
        "try:\n",
        "    print(\"Training DNN...\")\n",
        "    dnn_model = create_dnn(X_pca.shape[1], len(np.unique(y_encoded)))\n",
        "    dnn_model.fit(X_pca, y_encoded, epochs=20, batch_size=32, verbose=0)\n",
        "    y_pred_train_dnn = np.argmax(dnn_model.predict(X_pca), axis=1)\n",
        "    y_pred_dnn = np.argmax(dnn_model.predict(X_test_pca), axis=1)\n",
        "    f1_train_dnn = f1_score(y_encoded, y_pred_train_dnn, average='weighted')\n",
        "    f1_test_dnn = f1_score(y_true_encoded, y_pred_dnn, average='weighted')\n",
        "    results[\"DNN\"] = {\"Train F1\": f1_train_dnn, \"Test F1\": f1_test_dnn}\n",
        "    print(f\"DNN - Train F1: {f1_train_dnn:.4f}, Test F1: {f1_test_dnn:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"DNN failed: {e}\")\n",
        "\n",
        "# ANN 학습 및 평가\n",
        "try:\n",
        "    print(\"Training ANN...\")\n",
        "    ann_model = create_ann(X_pca.shape[1], len(np.unique(y_encoded)))\n",
        "    ann_model.fit(X_pca, y_encoded, epochs=20, batch_size=32, verbose=0)\n",
        "    y_pred_train_ann = np.argmax(ann_model.predict(X_pca), axis=1)\n",
        "    y_pred_ann = np.argmax(ann_model.predict(X_test_pca), axis=1)\n",
        "    f1_train_ann = f1_score(y_encoded, y_pred_train_ann, average='weighted')\n",
        "    f1_test_ann = f1_score(y_true_encoded, y_pred_ann, average='weighted')\n",
        "    results[\"ANN\"] = {\"Train F1\": fを表示_train_ann, \"Test F1\": f1_test_ann}\n",
        "    print(f\"ANN - Train F1: {f1_train_ann:.4f}, Test F1: {f1_test_ann:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"ANN failed: {e}\")\n",
        "\n",
        "# MLP 학습 및 평가\n",
        "try:\n",
        "    print(\"Training MLP...\")\n",
        "    mlp_model = create_mlp(X_pca.shape[1], len(np.unique(y_encoded)))\n",
        "    mlp_model.fit(X_pca, y_encoded, epochs=20, batch_size=32, verbose=0)\n",
        "    y_pred_train_mlp = np.argmax(mlp_model.predict(X_pca), axis=1)\n",
        "    y_pred_mlp = np.argmax(mlp_model.predict(X_test_pca), axis=1)\n",
        "    f1_train_mlp = f1_score(y_encoded, y_pred_train_mlp, average='weighted')\n",
        "    f1_test_mlp = f1_score(y_true_encoded, y_pred_mlp, average='weighted')\n",
        "    results[\"MLP\"] = {\"Train F1\": f1_train_mlp, \"Test F1\": f1_test_mlp}\n",
        "    print(f\"MLP - Train F1: {f1_train_mlp:.4f}, Test F1: {f1_test_mlp:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"MLP failed: {e}\")\n",
        "\n",
        "# CNN 학습 및 평가\n",
        "try:\n",
        "    print(\"Training CNN...\")\n",
        "    X_pca_cnn = X_pca.reshape((X_pca.shape[0], X_pca.shape[1], 1))\n",
        "    X_test_pca_cnn = X_test_pca.reshape((X_test_pca.shape[0], X_test_pca.shape[1], 1))\n",
        "    cnn_model = create_cnn(X_pca.shape[1], len(np.unique(y_encoded)))\n",
        "    cnn_model.fit(X_pca_cnn, y_encoded, epochs=20, batch_size=32, verbose=0)\n",
        "    y_pred_train_cnn = np.argmax(cnn_model.predict(X_pca_cnn), axis=1)\n",
        "    y_pred_cnn = np.argmax(cnn_model.predict(X_test_pca_cnn), axis=1)\n",
        "    f1_train_cnn = f1_score(y_encoded, y_pred_train_cnn, average='weighted')\n",
        "    f1_test_cnn = f1_score(y_true_encoded, y_pred_cnn, average='weighted')\n",
        "    results[\"CNN\"] = {\"Train F1\": f1_train_cnn, \"Test F1\": f1_test_cnn}\n",
        "    print(f\"CNN - Train F1: {f1_train_cnn:.4f}, Test F1: {f1_test_cnn:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"CNN failed: {e}\")\n",
        "\n",
        "# 결과 정리\n",
        "print(\"\\n=== Model Performance ===\")\n",
        "for name, scores in results.items():\n",
        "    print(f\"{name}: Train F1 = {scores['Train F1']:.4f}, Test F1 = {scores['Test F1']:.4f}, \"\n",
        "          f\"Difference = {abs(scores['Train F1'] - scores['Test F1']):.4f}\")\n",
        "\n",
        "best_model = max(results, key=lambda x: results[x][\"Test F1\"])\n",
        "print(f\"\\nBest Model: {best_model} with Test F1-Score: {results[best_model]['Test F1']:.4f}\")\n",
        "\n",
        "# 결과 DataFrame\n",
        "results_df = pd.DataFrame(\n",
        "    [(name, scores[\"Train F1\"], scores[\"Test F1\"]) for name, scores in results.items()],\n",
        "    columns=[\"Model\", \"Train F1-Score\", \"Test F1-Score\"]\n",
        ")\n",
        "print(\"\\nResults Table:\\n\", results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6rL0X99LBb2",
        "outputId": "8aa5bffd-6b99-4455-c601-58e490675c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original features: 851, After removing constants: 716\n",
            "SelectKBest (cleaned) - Train F1: 0.9276, Test F1: 0.8420\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 상수 피처 제거\n",
        "var_thresh = VarianceThreshold(threshold=0)  # 분산 0인 피처 제거\n",
        "X_cleaned = var_thresh.fit_transform(X_scaled)\n",
        "X_test_cleaned = var_thresh.transform(X_test_scaled)\n",
        "print(f\"Original features: {X_scaled.shape[1]}, After removing constants: {X_cleaned.shape[1]}\")\n",
        "\n",
        "# SelectKBest 적용\n",
        "selector = SelectKBest(score_func=f_classif, k=10)\n",
        "X_selected = selector.fit_transform(X_cleaned, y_encoded)\n",
        "X_test_selected = selector.transform(X_test_cleaned)\n",
        "\n",
        "# CatBoost 학습 및 평가\n",
        "model = CatBoostClassifier(verbose=0, random_state=42)\n",
        "model.fit(X_selected, y_encoded)\n",
        "y_pred_train = model.predict(X_selected)\n",
        "y_pred = model.predict(X_test_selected)\n",
        "f1_train = f1_score(y_encoded, y_pred_train, average='weighted')\n",
        "f1_test = f1_score(y_true_encoded, y_pred, average='weighted')\n",
        "print(f\"SelectKBest (cleaned) - Train F1: {f1_train:.4f}, Test F1: {f1_test:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-EOClrzLtD4",
        "outputId": "1fdd5038-6c75-4498-ed65-672b7ffd3edb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tree Importance - Test F1: 0.8479\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_scaled, y_encoded)\n",
        "importances = rf.feature_importances_\n",
        "top_k_indices = np.argsort(importances)[-10:]  # 상위 10개\n",
        "X_rf = X_scaled[:, top_k_indices]\n",
        "X_test_rf = X_test_scaled[:, top_k_indices]\n",
        "\n",
        "model = CatBoostClassifier(verbose=0, random_state=42)\n",
        "model.fit(X_rf, y_encoded)\n",
        "y_pred = model.predict(X_test_rf)\n",
        "f1_test = f1_score(y_true_encoded, y_pred, average='weighted')\n",
        "print(f\"Tree Importance - Test F1: {f1_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vMa1m8GO9yN"
      },
      "source": [
        "# Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtEOTxLPQgZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec01285e-54ab-4412-ff9b-de879b25e5b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.14.1)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.14.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.14.1)\n"
          ]
        }
      ],
      "source": [
        "# Colab에서 필요한 패키지 설치\n",
        "!pip install xgboost\n",
        "!pip install catboost\n",
        "!pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As5Q4ni4R7hA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "96a39890-f6c1-4183-c608-24aae91ad48b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "blosc2 3.3.0 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "071e844857b8415ba5a390564e070719"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install numpy==1.24.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSA5YpFGZSeU"
      },
      "source": [
        "# 조합 1: CatBoost + Logistic Regression + MLP 메타:Logistic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 임포트\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "# 데이터 준비\n",
        "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]\n",
        "X = train_sampled_df[feature_cols].copy()\n",
        "y = train_sampled_df[\"Segment\"].copy()\n",
        "\n",
        "# 타깃 라벨 인코딩 (학습과 테스트 데이터의 일관성 보장)\n",
        "combined_segments = pd.concat([train_sampled_df[\"Segment\"], evaluation_df[\"Segment\"]])\n",
        "le_target = LabelEncoder()\n",
        "le_target.fit(combined_segments)\n",
        "y_encoded = le_target.transform(train_sampled_df[\"Segment\"])\n",
        "\n",
        "# 테스트 데이터 준비\n",
        "X_test = evaluation_df[feature_cols].copy()\n",
        "y_true_encoded = le_target.transform(evaluation_df[\"Segment\"])\n",
        "\n",
        "# 범주형 피처 인코딩\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "encoders = {}\n",
        "for col in categorical_features:\n",
        "    le_train = LabelEncoder()\n",
        "    X[col] = X[col].astype(str)\n",
        "    X[col] = le_train.fit_transform(X[col])\n",
        "    encoders[col] = le_train\n",
        "\n",
        "    X_test[col] = X_test[col].astype(str)\n",
        "    unseen_labels_val = set(X_test[col]) - set(le_train.classes_)\n",
        "    if unseen_labels_val:\n",
        "        le_train.classes_ = np.append(le_train.classes_, list(unseen_labels_val))\n",
        "    X_test[col] = le_train.transform(X_test[col])\n",
        "\n",
        "# 수치형 피처 스케일링\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "scaler = StandardScaler()\n",
        "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
        "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
        "\n",
        "# 메모리 정리\n",
        "gc.collect()\n",
        "\n",
        "# 메모리 정리\n",
        "gc.collect()\n",
        "\n",
        "# 스태킹 조합 1 정의\n",
        "base_models_1 = [\n",
        "    ('catboost', CatBoostClassifier(verbose=0, random_state=42)),\n",
        "    ('logreg', LogisticRegression(max_iter=2000, random_state=42)),\n",
        "    ('mlp', MLPClassifier(hidden_layer_sizes=(256, 128, 64), max_iter=20, random_state=42))\n",
        "]\n",
        "meta_model_1 = LogisticRegression(max_iter=2000, random_state=42)\n",
        "\n",
        "# 학습 및 평가\n",
        "print(\"Training Stacking 조합 1...\")\n",
        "stacking_model_1 = StackingClassifier(estimators=base_models_1, final_estimator=meta_model_1, cv=5)\n",
        "stacking_model_1.fit(X, y_encoded)\n",
        "y_pred_1 = stacking_model_1.predict(X_test)\n",
        "f1_1 = f1_score(y_true_encoded, y_pred_1, average='weighted')\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"조합 1 Test F1-Score = {f1_1:.4f}\")"
      ],
      "metadata": {
        "id": "cQVVj-DClePY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 조합1-1 CatBoost + Logistic Regression + MLP  메타:Catboost"
      ],
      "metadata": {
        "id": "Pw9e7cBAjcbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 임포트\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "# 데이터 준비\n",
        "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]  # ID와 Segment 제외한 피처 선택\n",
        "X = train_sampled_df[feature_cols].copy()\n",
        "y = train_sampled_df[\"Segment\"].copy()\n",
        "\n",
        "# 타깃 라벨 인코딩 (학습과 테스트 데이터의 일관성 보장)\n",
        "combined_segments = pd.concat([train_sampled_df[\"Segment\"], evaluation_df[\"Segment\"]])\n",
        "le_target = LabelEncoder()\n",
        "le_target.fit(combined_segments)\n",
        "y_encoded = le_target.transform(train_sampled_df[\"Segment\"])\n",
        "\n",
        "# 테스트 데이터 준비\n",
        "X_test = evaluation_df[feature_cols].copy()\n",
        "y_true_encoded = le_target.transform(evaluation_df[\"Segment\"])\n",
        "\n",
        "# 범주형 피처 인코딩\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "encoders = {}\n",
        "for col in categorical_features:\n",
        "    le_train = LabelEncoder()\n",
        "    X[col] = X[col].astype(str)\n",
        "    X[col] = le_train.fit_transform(X[col])\n",
        "    encoders[col] = le_train\n",
        "\n",
        "    X_test[col] = X_test[col].astype(str)\n",
        "    X_test[col] = le_train.transform(X_test[col])  # 새로운 레이블 없으므로 바로 변환\n",
        "\n",
        "# 데이터 스케일링\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "scaler = StandardScaler()\n",
        "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
        "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
        "\n",
        "# 메모리 정리\n",
        "gc.collect()\n",
        "\n",
        "# 스태킹 조합 1 정의\n",
        "base_models_1 = [\n",
        "    ('catboost', CatBoostClassifier(verbose=0, random_state=42)),\n",
        "    ('logreg', LogisticRegression(max_iter=2000, random_state=42)),\n",
        "    ('mlp', MLPClassifier(hidden_layer_sizes=(256, 128, 64), max_iter=200, early_stopping=True, random_state=42))\n",
        "]\n",
        "meta_model_1 = CatBoostClassifier(verbose=0, random_state=42)\n",
        "\n",
        "# 학습 및 평가\n",
        "print(\"스태킹 조합 1-1 학습 중...\")\n",
        "stacking_model_1_2 = StackingClassifier(estimators=base_models_1, final_estimator=meta_model_1, cv=5)\n",
        "stacking_model_1_2.fit(X, y_encoded)\n",
        "y_pred_1 = stacking_model_1_2.predict(X_test)\n",
        "f1_1 = f1_score(y_true_encoded, y_pred_1, average='weighted')\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"조합 1-1 테스트 F1-점수 = {f1_1:.4f}\")\n",
        "print(\"\\n분류 보고서:\")\n",
        "print(classification_report(y_true_encoded, y_pred_1, target_names=le_target.classes_))\n",
        "\n",
        "# 추가 메모리 정리\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "huf9EvZpjgmR",
        "outputId": "8031181d-8f93-4047-bd37-8215a835020c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "스태킹 조합 1-1 학습 중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (4) does not match total number of classes (5). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (4) does not match total number of classes (5). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (4) does not match total number of classes (5). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "조합 1-1 테스트 F1-점수 = 0.8911\n",
            "\n",
            "분류 보고서:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Number of classes, 4, does not match size of target_names, 5. Try specifying the labels parameter",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a6ec97f44400>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"조합 1-1 테스트 F1-점수 = {f1_1:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n분류 보고서:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mle_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# 추가 메모리 정리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2691\u001b[0m             )\n\u001b[1;32m   2692\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2694\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of classes, 4, does not match size of target_names, 5. Try specifying the labels parameter"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 조합1-2 CatBoost + Logistic Regression + MLP + SVM + KNN 메타:Catboost"
      ],
      "metadata": {
        "id": "3Ez9gf45Bwu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 데이터 준비\n",
        "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]\n",
        "X = train_sampled_df[feature_cols].copy()\n",
        "y = train_sampled_df[\"Segment\"].copy()\n",
        "\n",
        "# 타깃 라벨 인코딩 (학습과 테스트 데이터의 일관성 보장)\n",
        "combined_segments = pd.concat([train_sampled_df[\"Segment\"], evaluation_df[\"Segment\"]])\n",
        "le_target = LabelEncoder()\n",
        "le_target.fit(combined_segments)\n",
        "y_encoded = le_target.transform(train_sampled_df[\"Segment\"])\n",
        "\n",
        "# 테스트 데이터 준비\n",
        "X_test = evaluation_df[feature_cols].copy()\n",
        "y_true_encoded = le_target.transform(evaluation_df[\"Segment\"])\n",
        "\n",
        "# 범주형 피처 인코딩\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "encoders = {}\n",
        "for col in categorical_features:\n",
        "    le_train = LabelEncoder()\n",
        "    X[col] = X[col].astype(str)\n",
        "    X[col] = le_train.fit_transform(X[col])\n",
        "    encoders[col] = le_train\n",
        "\n",
        "    X_test[col] = X_test[col].astype(str)\n",
        "    unseen_labels_val = set(X_test[col]) - set(le_train.classes_)\n",
        "    if unseen_labels_val:\n",
        "        le_train.classes_ = np.append(le_train.classes_, list(unseen_labels_val))\n",
        "    X_test[col] = le_train.transform(X_test[col])\n",
        "\n",
        "# 데이터 스케일링\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "scaler = StandardScaler()\n",
        "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
        "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
        "\n",
        "# 메모리 정리\n",
        "gc.collect()\n",
        "\n",
        "# Base 모델 정의\n",
        "base_models = [\n",
        "    ('catboost', CatBoostClassifier(verbose=0, random_state=42)),\n",
        "    ('logreg', LogisticRegression(max_iter=2000, random_state=42)),\n",
        "    ('mlp', MLPClassifier(hidden_layer_sizes=(256, 128, 64), max_iter=20, random_state=42)),\n",
        "    ('svc', SVC(probability=True, random_state=42)),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "]\n",
        "\n",
        "# 메타 모델 정의 (CatBoost)\n",
        "meta_model = CatBoostClassifier(verbose=0, random_state=42)\n",
        "\n",
        "# 스태킹 모델 구성\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 학습\n",
        "print(\"Training Stacking Ensemble...\")\n",
        "stacking_model.fit(X, y_encoded)\n",
        "\n",
        "# 예측\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "\n",
        "# 평가\n",
        "f1 = f1_score(y_true_encoded, y_pred, average='weighted')\n",
        "print(f\" Final Test Weighted F1-Score = {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_qcwZG1AtLM",
        "outputId": "7b86a26c-fc06-4aa1-9532-640596f2de0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Stacking Ensemble...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Final Test Weighted F1-Score = 0.8910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct-6ISquZTCP"
      },
      "source": [
        "# 조합2: XGBoost + Random Forest + DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsD8YBjgZdxA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "7fe24a02-ea3e-46e5-84af-2e7f105b0388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Stacking 조합 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3], got [0 2 3 4]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2538e50a880f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Stacking 조합 2...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mstacking_model_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_models_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_model_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mstacking_model_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0my_pred_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacking_model_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mf1_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     @available_if(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             predictions = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 delayed(cross_val_predict)(\n\u001b[1;32m    256\u001b[0m                     \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m     predictions = parallel(\n\u001b[0m\u001b[1;32m   1248\u001b[0m         delayed(_fit_and_predict)(\n\u001b[1;32m   1249\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_predict\u001b[0;34m(estimator, X, y, train, test, fit_params, method)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpected_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m             ):\n\u001b[0;32m-> 1559\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1560\u001b[0m                     \u001b[0;34mf\"Invalid classes inferred from unique values of `y`.  \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                     \u001b[0;34mf\"Expected: {expected_classes}, got {classes}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3], got [0 2 3 4]"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 임포트\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "# 데이터 준비\n",
        "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]\n",
        "X = train_sampled_df[feature_cols].copy()\n",
        "y = train_sampled_df[\"Segment\"].copy()\n",
        "\n",
        "# 타깃 라벨 인코딩 (학습과 테스트 데이터의 일관성 보장)\n",
        "combined_segments = pd.concat([train_sampled_df[\"Segment\"], evaluation_df[\"Segment\"]])\n",
        "le_target = LabelEncoder()\n",
        "le_target.fit(combined_segments)\n",
        "y_encoded = le_target.transform(train_sampled_df[\"Segment\"])\n",
        "\n",
        "# 테스트 데이터 준비\n",
        "X_test = evaluation_df[feature_cols].copy()\n",
        "y_true_encoded = le_target.transform(evaluation_df[\"Segment\"])\n",
        "\n",
        "# 범주형 피처 인코딩\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "encoders = {}\n",
        "for col in categorical_features:\n",
        "    le_train = LabelEncoder()\n",
        "    X[col] = le_train.fit_transform(X[col])\n",
        "    encoders[col] = le_train\n",
        "    X_test[col] = X_test[col].fillna('missing').astype(str)\n",
        "    unseen_labels_val = set(X_test[col]) - set(le_train.classes_)\n",
        "    if unseen_labels_val:\n",
        "        le_train.classes_ = np.append(le_train.classes_, list(unseen_labels_val))\n",
        "    X_test[col] = le_train.transform(X_test[col])\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 메모리 정리\n",
        "gc.collect()\n",
        "\n",
        "# 스태킹 조합 2 정의\n",
        "base_models_2 = [\n",
        "    ('xgboost', xgb.XGBClassifier(tree_method='hist', random_state=42)),\n",
        "    ('rf', RandomForestClassifier(random_state=42)),\n",
        "    ('dnn', MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=20, random_state=42))\n",
        "]\n",
        "meta_model_2 = LogisticRegression(max_iter=2000, random_state=42)\n",
        "\n",
        "# 학습 및 평가\n",
        "print(\"Training Stacking 조합 2...\")\n",
        "stacking_model_2 = StackingClassifier(estimators=base_models_2, final_estimator=meta_model_2, cv=5)\n",
        "stacking_model_2.fit(X_scaled, y_encoded)\n",
        "y_pred_2 = stacking_model_2.predict(X_test_scaled)\n",
        "f1_2 = f1_score(y_true_encoded, y_pred_2, average='weighted')\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"조합 2 Test F1-Score = {f1_2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrU0v6JVZTgR"
      },
      "source": [
        "# 조합3: CatBoost + SVM + LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djei8nJ5Zeso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38c8084a-c6cc-4633-ad5c-98ea738a7c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Stacking 조합 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113747 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 75813\n",
            "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 671\n",
            "[LightGBM] [Info] Start training from score -7.600902\n",
            "[LightGBM] [Info] Start training from score -9.903488\n",
            "[LightGBM] [Info] Start training from score -2.934637\n",
            "[LightGBM] [Info] Start training from score -1.900793\n",
            "[LightGBM] [Info] Start training from score -0.227089\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (4) does not match total number of classes (5). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (4) does not match total number of classes (5). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090185 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 73903\n",
            "[LightGBM] [Info] Number of data points in the train set: 16000, number of used features: 664\n",
            "[LightGBM] [Info] Start training from score -7.600902\n",
            "[LightGBM] [Info] Start training from score -9.680344\n",
            "[LightGBM] [Info] Start training from score -2.933932\n",
            "[LightGBM] [Info] Start training from score -1.900877\n",
            "[LightGBM] [Info] Start training from score -0.227136\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089183 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 73894\n",
            "[LightGBM] [Info] Number of data points in the train set: 16000, number of used features: 664\n",
            "[LightGBM] [Info] Start training from score -7.600902\n",
            "[LightGBM] [Info] Start training from score -9.680344\n",
            "[LightGBM] [Info] Start training from score -2.935108\n",
            "[LightGBM] [Info] Start training from score -1.900459\n",
            "[LightGBM] [Info] Start training from score -0.227136\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 73917\n",
            "[LightGBM] [Info] Number of data points in the train set: 16000, number of used features: 662\n",
            "[LightGBM] [Info] Start training from score -7.600902\n",
            "[LightGBM] [Info] Start training from score -9.680344\n",
            "[LightGBM] [Info] Start training from score -2.935108\n",
            "[LightGBM] [Info] Start training from score -1.900877\n",
            "[LightGBM] [Info] Start training from score -0.227057\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090139 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 73956\n",
            "[LightGBM] [Info] Number of data points in the train set: 16000, number of used features: 663\n",
            "[LightGBM] [Info] Start training from score -7.600902\n",
            "[LightGBM] [Info] Start training from score -9.680344\n",
            "[LightGBM] [Info] Start training from score -2.935108\n",
            "[LightGBM] [Info] Start training from score -1.900877\n",
            "[LightGBM] [Info] Start training from score -0.227057\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156189 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 73923\n",
            "[LightGBM] [Info] Number of data points in the train set: 16000, number of used features: 663\n",
            "[LightGBM] [Info] Start training from score -7.600902\n",
            "[LightGBM] [Info] Start training from score -2.933932\n",
            "[LightGBM] [Info] Start training from score -1.900877\n",
            "[LightGBM] [Info] Start training from score -0.227057\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (4) does not match total number of classes (5). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "조합 3 Test F1-Score = 0.8887\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 임포트\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "# 데이터 준비\n",
        "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]\n",
        "X = train_sampled_df[feature_cols].copy()\n",
        "y = train_sampled_df[\"Segment\"].copy()\n",
        "\n",
        "# 타깃 라벨 인코딩 (학습과 테스트 데이터의 일관성 보장)\n",
        "combined_segments = pd.concat([train_sampled_df[\"Segment\"], evaluation_df[\"Segment\"]])\n",
        "le_target = LabelEncoder()\n",
        "le_target.fit(combined_segments)\n",
        "y_encoded = le_target.transform(train_sampled_df[\"Segment\"])\n",
        "\n",
        "# 테스트 데이터 준비\n",
        "X_test = evaluation_df[feature_cols].copy()\n",
        "y_true_encoded = le_target.transform(evaluation_df[\"Segment\"])\n",
        "\n",
        "# 범주형 피처 인코딩\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "encoders = {}\n",
        "for col in categorical_features:\n",
        "    le_train = LabelEncoder()\n",
        "    X[col] = le_train.fit_transform(X[col])\n",
        "    encoders[col] = le_train\n",
        "    X_test[col] = X_test[col].fillna('missing').astype(str)\n",
        "    unseen_labels_val = set(X_test[col]) - set(le_train.classes_)\n",
        "    if unseen_labels_val:\n",
        "        le_train.classes_ = np.append(le_train.classes_, list(unseen_labels_val))\n",
        "    X_test[col] = le_train.transform(X_test[col])\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 메모리 정리\n",
        "gc.collect()\n",
        "\n",
        "# 스태킹 조합 3 정의\n",
        "base_models_3 = [\n",
        "    ('catboost', CatBoostClassifier(verbose=0, random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)),\n",
        "    ('lgbm', LGBMClassifier(random_state=42))\n",
        "]\n",
        "meta_model_3 = LogisticRegression(max_iter=2000, random_state=42)\n",
        "\n",
        "# 학습 및 평가\n",
        "print(\"Training Stacking 조합 3...\")\n",
        "stacking_model_3 = StackingClassifier(estimators=base_models_3, final_estimator=meta_model_3, cv=5)\n",
        "stacking_model_3.fit(X_scaled, y_encoded)\n",
        "y_pred_3 = stacking_model_3.predict(X_test_scaled)\n",
        "f1_3 = f1_score(y_true_encoded, y_pred_3, average='weighted')\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"조합 3 Test F1-Score = {f1_3:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBFeUeyrZg18"
      },
      "source": [
        "# 조합4: Logistic Regression + Random Forest + Naive Bayes + CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9BTuwe1ZuxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb0a778-b490-4566-b56c-322a8cfd0efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Stacking 조합 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (4) does not match total number of classes (5). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (4) does not match total number of classes (5). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (4) does not match total number of classes (5). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:1380: RuntimeWarning: Number of classes in training fold (4) does not match total number of classes (5). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "조합 4 Test F1-Score = 0.8870\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 임포트\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "# 데이터 준비\n",
        "feature_cols = [col for col in train_sampled_df.columns if col not in [\"ID\", \"Segment\"]]\n",
        "X = train_sampled_df[feature_cols].copy()\n",
        "y = train_sampled_df[\"Segment\"].copy()\n",
        "\n",
        "# 타깃 라벨 인코딩 (학습과 테스트 데이터의 일관성 보장)\n",
        "combined_segments = pd.concat([train_sampled_df[\"Segment\"], evaluation_df[\"Segment\"]])\n",
        "le_target = LabelEncoder()\n",
        "le_target.fit(combined_segments)\n",
        "y_encoded = le_target.transform(train_sampled_df[\"Segment\"])\n",
        "\n",
        "# 테스트 데이터 준비\n",
        "X_test = evaluation_df[feature_cols].copy()\n",
        "y_true_encoded = le_target.transform(evaluation_df[\"Segment\"])\n",
        "\n",
        "# 범주형 피처 인코딩\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "encoders = {}\n",
        "for col in categorical_features:\n",
        "    le_train = LabelEncoder()\n",
        "    X[col] = le_train.fit_transform(X[col])\n",
        "    encoders[col] = le_train\n",
        "    X_test[col] = X_test[col].fillna('missing').astype(str)\n",
        "    unseen_labels_val = set(X_test[col]) - set(le_train.classes_)\n",
        "    if unseen_labels_val:\n",
        "        le_train.classes_ = np.append(le_train.classes_, list(unseen_labels_val))\n",
        "    X_test[col] = le_train.transform(X_test[col])\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 메모리 정리\n",
        "gc.collect()\n",
        "\n",
        "# 스태킹 조합 4 정의\n",
        "base_models_4 = [\n",
        "    ('logreg', LogisticRegression(max_iter=2000, random_state=42)),\n",
        "    ('rf', RandomForestClassifier(random_state=42)),\n",
        "    ('nb', GaussianNB()),\n",
        "    ('cnn', MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=20, random_state=42))\n",
        "]\n",
        "meta_model_4 = LogisticRegression(max_iter=2000, random_state=42)\n",
        "\n",
        "# 학습 및 평가\n",
        "print(\"Training Stacking 조합 4...\")\n",
        "stacking_model_4 = StackingClassifier(estimators=base_models_4, final_estimator=meta_model_4, cv=5)\n",
        "stacking_model_4.fit(X_scaled, y_encoded)\n",
        "y_pred_4 = stacking_model_4.predict(X_test_scaled)\n",
        "f1_4 = f1_score(y_true_encoded, y_pred_4, average='weighted')\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"조합 4 Test F1-Score = {f1_4:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Y8x3cRUBwG4x",
        "HgTVOKWRoGf-",
        "k3zIAUEOn-DW",
        "GBLEFGKhxc5_",
        "0vMa1m8GO9yN",
        "3Ez9gf45Bwu7"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}